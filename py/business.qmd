---
title: "Business spending"
format: html
---

```{python}
import pandas as pd
import requests
from bs4 import BeautifulSoup
import re
import geopandas as gpd
from shapely.geometry import Point
import matplotlib.pyplot as plt
import contextily as cx
from adjustText import adjust_text
import configparser
import psycopg2
import os
import pickle
import numpy as np
from sklearn.neighbors import BallTree, radius_neighbors_graph
from scipy.spatial import cKDTree
import janitor
import zipfile
import glob
from sqlalchemy import create_engine

config = configparser.ConfigParser()
config.read(os.path.join('..', 'db_config.ini'))

db_params = dict(config['postgresql'])

# Load DB config
config = configparser.ConfigParser()
config.read(os.path.join('..', 'db_config.ini'))
db_params = dict(config['postgresql'])

# Build SQLAlchemy connection string
conn_str = (
    f"postgresql+psycopg2://{db_params['user']}:{db_params['password']}"
    f"@{db_params['host']}:{db_params['port']}/{db_params['database']}"
)

# Create engine
engine = create_engine(conn_str)
```

```{python}
# Get postcodes in Haringey and Hackney (as a comparitor)
with engine.connect() as con:
    query = '''
            SELECT DISTINCT pcds, ladnm 
            FROM pcode_census21_lookup
            WHERE ladnm IN ('Haringey','Hackney')
            '''

    pcode_lookup = pd.read_sql(con=con, sql=query)

# Get the first part for pcode district
pcode_lookup['pcode_district'] = pcode_lookup['pcds'].str.split(' ').str[0]

pcode_lookup.to_pickle(os.path.join('..','data','pcode_lookup.pkl'))

# Get Haringey postcode boundaries
haringey_pcode_districts = pcode_lookup.loc[pcode_lookup['ladnm']=='Haringey','pcode_district'].unique().tolist()
pcodes_of_interest = pcode_lookup['pcode_district'].unique().tolist()
# Convert list to SQL-safe string
formatted_list = ', '.join([f"'{district}'" for district in haringey_pcode_districts])
with engine.connect() as con:
    query2 = f'''
                SELECT * 
                FROM postcode_district_boundaries
                WHERE district IN ({formatted_list})
                '''
    haringey_districts_gpd = gpd.read_postgis(query2, con=con, geom_col='geometry')

haringey_districts_gpd.to_pickle(os.path.join('..','data','pcode_districts_haringey.pkl'))

```

```{python}
# Get the cardspending data
file = os.path.join('..','data', 'card_data.pkl')
if os.path.isfile(file)==False:   
    url = 'https://www.nomisweb.co.uk/output/eop/postal_district_quarterly_indexed_map_data.zip'

    # Check if it's already downloaded
    basename = os.path.basename(url)
    path = os.path.join('..', 'data', basename)
    if os.path.isfile(path)==False:
        req = requests.get(url)
        with open(path, 'wb') as output_file:
            output_file.write(req.content)
    else:
        print('Data already acquired. Loading it')

    # if it's a zipped folder, unzip
    # define outpath as same as in path minus .zip
    out_path = os.path.splitext(path)[0] # This removes the .zip extension 

    # Create the extraction directory if it doesn't exist
    if not os.path.exists(out_path):
        os.makedirs(out_path)

    # unzip
    with zipfile.ZipFile(path, 'r') as zip_ref:
        zip_ref.extractall(out_path)

    # Use glob to get all CSV files in the directory
    files = glob.glob(os.path.join(out_path, '*.csv'))

    card_data = pd.read_csv(files[0])

    card_data.to_pickle(os.path.join('..','data','card_data.pkl'))
else:
    print('Card data already downloaded. Loading...')
    with open(file, "rb") as input_file:
        card_data = pickle.load(input_file)
    

# Split time period into year and quarter
# card_data[['year','quarter']] = card_data['time_period_value'].str.split('Q', expand=True).astype(int)

# card_data['year_month'] = pd.to_datetime(
#     card_data['year'].astype(str) + card_data['quarter'].map(month_map)
# ).dt.strftime('%Y-%m')

# Faster alternative
years = card_data['time_period_value'].str[:4].astype(int)
quarters = card_data['time_period_value'].str[-1].astype(int)
# Map quarter to month and construct the date string
month_map = np.array(['-03-01', '-06-01', '-09-01', '-12-01'])

# Vectorized concatenation
year_month_str = years.astype(str) + month_map[quarters - 1]

# Assign columns
card_data['year'] = years
card_data['quarter'] = quarters
card_data['year_month'] = year_month_str #pd.to_datetime(year_month_str).dt.strftime('%Y-%m')


# Subset to card data that involves postcodes of interest
card_data2 = card_data.loc[card_data['cardholder_location'].isin(pcodes_of_interest) | card_data['merchant_location'].isin(pcodes_of_interest)]

```

This script uses cardholder spending by postcode district to explore spending in the N17 (Tottenham) area and compares to other areas.

Cardholder spending data was acquired from NOMIS here: https://www.nomisweb.co.uk/sources/economy_ccs

Note that I believe the variable documentation on this page is inaccurate. Specifically, the definitions of cardholder_index_spend and merchant_index_spend are the wrong way round. The documentation describes the variables as follows: 

- time_period_value: Which time period the data relates to. Format: YYYYQQ
- Cardholder_location: A cardholder home postal district
- Merchant_location: The postal district where merchants are located and spend took place.
- Cardholder_index_spend: Proportion of total spend at merchants in the merchant location for cardholders based in the cardholder location.
- Merchant_index_spend: Proportion of total spend by cardholders based in the cardholder location at merchants based in the merchant location.

Based on the information that "The total value for each quarter at a cardholder or merchant location is indexed to 2019 Q1=100", we should expect that for Q1 in 2019, the sum of cardholder_index_spend for each merchant location should be approximately equal to 100. Similarly, for Q1 in 2019 the sum of merchant_index_spend for each cardholder_location should be approximately 100. This is not the case:

```{python}
q1_2019 = card_data.loc[card_data['time_period_value']=='2019Q1']
test1 = q1_2019.groupby(['time_period_value','merchant_location'])['cardholder_index_spend'].sum().reset_index()
test1.head(10)
```

```{python}
test2 = q1_2019.groupby(['time_period_value','cardholder_location'])['merchant_index_spend'].sum().reset_index()
test2.head(10)
```

But if we swap the definitions, the counts sum as expected. That is, in Q1 2019, the sum of cardholder_index_spend for each *cardholder_location* is approximately equal to 100, as is the sum of merchant_index_spend for each *merchant_location*.

```{python}
q1_2019 = card_data.loc[card_data['time_period_value']=='2019Q1']
test3 = q1_2019.groupby(['time_period_value','cardholder_location'])['cardholder_index_spend'].sum().reset_index()
test3.head(10)
```

``` {python}
test4 = q1_2019.groupby(['time_period_value','merchant_location'])['merchant_index_spend'].sum().reset_index()
test4.head(10)
```

I therefore believe (and the rest of this script assumes) that the indexes should instead be defined as:

- Cardholder_index_spend: Proportion of total spend by cardholders based in the cardholder location at merchants based in the merchant location.
    - This can be understood as: "Of all the money spent by cardholders from a specific location (Cardholder_location), how much was spent at merchants in a specific location (Merchant_location)?" 
- Merchant_index_spend: Proportion of total spend at merchants in the merchant location for cardholders based in the cardholder location.
    - This can be understood as: "Of all the money spent at merchants in a specific location (Merchant_location), how much came from cardholders who live in a specific location (Cardholder_location)?"


# Test plot to visualise districts

These seem like an OK size for use and differentiate areas in Haringey reasonably well

```{python}
haringey_districts_gpd['centroid'] = haringey_districts_gpd.geometry.centroid

fig, ax = plt.subplots(1,1, figsize = [8,8])

haringey_districts_gpd.plot(ax=ax, alpha=0.5, edgecolor='black')
# Add labels at centroids
for idx, row in haringey_districts_gpd.iterrows():
    x = row['centroid'].x
    y = row['centroid'].y
    label = row['district']  
    ax.text(x, y, label, fontsize=8, ha='center', va='center', color='white')
cx.add_basemap(ax, crs = haringey_districts_gpd.crs, source=cx.providers.OpenStreetMap.Mapnik)
```

## Spending at Northumberland Park

Let's look at spending by distance. To do this we first need to calculate the distance of each postcode from NP.

```{python}
n17_merchant = card_data2.loc[card_data2['merchant_location'] == 'N17']
# Get a list of all the postcodes that feature in the NP merchant data
unique_combinations = n17_merchant.drop_duplicates(subset=['cardholder_location','merchant_location'])[['cardholder_location','merchant_location']]

unique_combinations_list = np.unique(unique_combinations.values.flatten()).tolist()

formatted_list = ', '.join([f"'{district}'" for district in unique_combinations_list])

with engine.connect() as con:
    query2 = f'''
             SELECT * 
             FROM postcode_district_boundaries
             WHERE district IN ({formatted_list})
             '''
    geoms_of_interest = gpd.read_postgis(query2, con=con, geom_col='geometry')

geoms_of_interest.to_pickle(os.path.join('..','data','geoms_of_interest1.pkl'))


# Get centroids of these areas
geoms_of_interest['centroid'] = geoms_of_interest.geometry.centroid
# Convert centroids to wgs84 in two steps using geoseries (this is becuase geodesic needs wgs84)
centroids_gs = gpd.GeoSeries(geoms_of_interest['centroid'], crs=geoms_of_interest.crs)
centroids_wgs84 = centroids_gs.to_crs(epsg=4326)
# Add the new crs centroids back into geoms_of_interest
geoms_of_interest['centroid'] = centroids_wgs84

# Join geometries for cardholders and merchants
df = unique_combinations.merge(geoms_of_interest[['district','centroid']].rename(columns={
    'district': 'cardholder_location',
    'centroid': 'cardholder_centroid'
}), on='cardholder_location', how='left')

df = df.merge(geoms_of_interest[['district','centroid']].rename(columns={
    'district': 'merchant_location',
    'centroid': 'merchant_centroid'
}), on='merchant_location', how='left')

# Now calculate the distance
from geopy.distance import geodesic

# Calculate the distance using a function
def calc_distance(row):
    ch_centroid = row['cardholder_centroid']
    m_centroid = row['merchant_centroid']

    if ch_centroid is not None and m_centroid is not None:
        # Make sure they are shapely points
        if hasattr(ch_centroid, 'x') and hasattr(m_centroid, 'x'):
            point1 = (ch_centroid.y, ch_centroid.x)  # (lat, lon)
            point2 = (m_centroid.y, m_centroid.x)
            return geodesic(point1, point2).kilometers
    return None

df['distance_km'] = df.apply(calc_distance, axis=1)

```

```{python}
# Plot for 2023
df2 = df.merge(n17_merchant, how='right',on=['merchant_location','cardholder_location'])

df2_2023 = df2.loc[df2['year']==2023]
# Get the mean of spend for each cardholder location
df2_2023['ch_avg_2023'] = df2_2023.groupby('cardholder_location')['cardholder_index_spend'].transform('mean')
df2_2023 = df2_2023.drop_duplicates(subset=['cardholder_location'])

# Specify a top percentage for labelling
top_pc = 1
avg_2023 = np.mean(df2_2023['ch_avg_2023'])
top_1pc = np.quantile(df2_2023['ch_avg_2023'], q=[1-(top_pc/100)])[0]

# plot to explore
fig, ax = plt.subplots(figsize = [8,8])
# Create the scatter plot
ax.scatter(x=df2_2023['distance_km'], y=df2_2023['ch_avg_2023'])

ax.set_xlabel('Distance (km)')
ax.set_ylabel('Average Proportion of Cardholder Spend in N17 (2023)')
ax.set_title('Scatter Plot of Distance vs. Average Cardholder Spend');

# Add labels where ch_avg_2023 > 1
texts = []
for _, row in df2_2023[df2_2023['ch_avg_2023'] > top_1pc].iterrows():
    texts.append(ax.text(row['distance_km'], row['ch_avg_2023'], row['cardholder_location'], fontsize=8, alpha=0.7))

# Automatically adjust positions to prevent overlap
adjust_text(texts, ax=ax, arrowprops=dict(arrowstyle='-', color='gray', lw=0.5))

plt.axhline(y=avg_2023, color='r', linestyle='dashed')
ax.text(
    ax.get_xlim()[1],
    ax.get_ylim()[0] * 4,
    s='Dashed red line indicates the average proportion for 2023 (' + str(round(avg_2023, 2)) + ')'\
        '\nLabelled areas are the top ' + str(top_pc) + '% of cardholder postcodes',
    size=8,
    ha='right'
);

```

Exploring the areas with above 1% spend

```{python}
top_pc_districts = df2_2023.loc[df2_2023['ch_avg_2023'] > top_1pc, 'cardholder_location'].tolist()
# Safely convert list to SQL-safe string
formatted_list = ', '.join([f"'{district}'" for district in top_pc_districts])
with engine.connect() as con:
    query2 = f'''
             SELECT * 
             FROM postcode_district_boundaries
             WHERE district IN ({formatted_list})
             '''
    top_pc_gdf = gpd.read_postgis(query2, con=con, geom_col='geometry')

top_pc_gdf.to_pickle(os.path.join('..','data','top_pc_gdf.pkl'))

top_pc_gdf['centroid'] = top_pc_gdf.geometry.centroid


combined1 = top_pc_gdf.merge(df2_2023,how='left',left_on='district',right_on='cardholder_location')

fig, ax = plt.subplots(1,1, figsize = [8,8])

combined1.plot(ax=ax, column='ch_avg_2023', legend=True, legend_kwds={'label': 'Average proprotion of spending in N17'}, alpha=0.5, edgecolor='black')
# Add labels at centroids
for idx, row in combined1.iterrows():
    x = row['centroid'].x
    y = row['centroid'].y
    label = row['district']  
    ax.text(x, y, label, fontsize=8, ha='center', va='center', color='black')
cx.add_basemap(ax, crs = combined1.crs, source=cx.providers.OpenStreetMap.Mapnik)
ax.set_axis_off()
```

Focusing on areas in London

```{python}
top_pc_districts3 = df2_2023.loc[(df2_2023['ch_avg_2023'] > top_1pc) & (df2_2023['distance_km'] < 20), 'cardholder_location'].tolist()
# Safely convert list to SQL-safe string
formatted_list = ', '.join([f"'{district}'" for district in top_pc_districts3])

with engine.connect() as con:
    query2 = f'''
             SELECT * 
             FROM postcode_district_boundaries
             WHERE district IN ({formatted_list})
             '''
    top_pc_gdf3 = gpd.read_postgis(query2, con=con, geom_col='geometry')

top_pc_gdf3.to_pickle(os.path.join('..','data','top_pc_gdf3.pkl'))

top_pc_gdf3['centroid'] = top_pc_gdf3.geometry.centroid

combined3 = top_pc_gdf3.merge(df2_2023, how='left',left_on='district',right_on='cardholder_location')

fig, ax = plt.subplots(1,1, figsize = [8,8])

combined3.plot(ax=ax, column='ch_avg_2023', legend=True, legend_kwds={'label': 'Average proprotion of spending in N17'}, alpha=0.5, edgecolor='black')
# Add labels at centroids
for idx, row in combined3.iterrows():
    x = row['centroid'].x
    y = row['centroid'].y
    label = row['district']  
    ax.text(x, y, label, fontsize=8, ha='center', va='center', color='black')
cx.add_basemap(ax, crs = combined3.crs, source=cx.providers.OpenStreetMap.Mapnik)
ax.set_axis_off()
```

Plot all areas just for info

```{python}
all_districts = df2_2023['cardholder_location'].drop_duplicates().tolist()
# Safely convert list to SQL-safe string
formatted_list = ', '.join([f"'{district}'" for district in all_districts])

with engine.connect() as con:
    query2 = f'''
             SELECT * 
             FROM postcode_district_boundaries
             WHERE district IN ({formatted_list})
             '''
    all_districts_gdf = gpd.read_postgis(query2, con=con, geom_col='geometry')

all_districts_gdf.to_pickle(os.path.join('..','data','top_pc_gdf.pkl'))

all_districts_gdf['centroid'] = all_districts_gdf.geometry.centroid

combined4 = all_districts_gdf.merge(df2_2023,how='left',left_on='district',right_on='cardholder_location')

fig, ax = plt.subplots(1,1, figsize = [8,8])

combined4.plot(ax=ax, column='ch_avg_2023', legend=True, legend_kwds={'label': 'Average proportion of spending in N17'}, alpha=0.5, edgecolor='black')
# Add labels at centroids
# for idx, row in combined4.iterrows():
#     x = row['centroid'].x
#     y = row['centroid'].y
#     label = row['district']  
#     ax.text(x, y, label, fontsize=8, ha='center', va='center', color='black')
cx.add_basemap(ax, crs = combined4.crs, source=cx.providers.OpenStreetMap.Mapnik)
ax.set_axis_off()
```


Plot N17 over time (the first quarter of each year)

```{python}
years = sorted(df2['year'].unique().tolist())
top_pc = 5
n_years = len(years)

# Prepare data for map plots (as in your second code block)
yearly_combined = {}
with engine.connect() as con:
    for year in years:
        df_year = df2[(df2['year'] == year) & (df2['quarter'] == 1)].copy()
        df_year = df_year.drop_duplicates(subset=['cardholder_location'])

        top_threshold = np.quantile(df_year['merchant_index_spend'], 1 - (top_pc / 100))
        top_districts = df_year[df_year['merchant_index_spend'] >= top_threshold]['cardholder_location'].tolist()
        formatted_list = ', '.join([f"'{district}'" for district in top_districts])
        query = f'''
            SELECT * 
            FROM postcode_district_boundaries
            WHERE district IN ({formatted_list})
        '''
        gdf = gpd.read_postgis(query, con=con, geom_col='geometry')
        gdf['centroid'] = gdf.geometry.centroid
        combined = gdf.merge(df_year, how='left', left_on='district', right_on='cardholder_location')
        yearly_combined[year] = combined

output_dir = os.path.join('..','data')
# Pickle the dictionary
with open(os.path.join(output_dir, 'yearly_combined.pkl'), 'wb') as f:
    pickle.dump(yearly_combined, f)

# Global color scale
all_spend_values = pd.concat([gdf['merchant_index_spend'] for gdf in yearly_combined.values()])
global_vmin = all_spend_values.min()
global_vmax = all_spend_values.max()

# Create 5x2 subplot
fig, axes = plt.subplots(nrows=n_years, ncols=2, figsize=(16, 6 * n_years))

# Ensure axes is always 2D
if n_years == 1:
    axes = np.array([axes])

for i, year in enumerate(years):
    ax_scatter = axes[i, 0]
    ax_map = axes[i, 1]

    ### LEFT: SCATTER PLOT ###
    df_year = df2[(df2['year'] == year) & (df2['quarter'] == 1)].copy()
    avg_val = df_year['merchant_index_spend'].mean()

    ax_scatter.scatter(df_year['distance_km'], df_year['merchant_index_spend'], alpha=0.6)
    ax_scatter.axhline(y=avg_val, color='r', linestyle='dashed')

    top_threshold = np.quantile(df_year['merchant_index_spend'], 1 - (top_pc / 100))
    top_df = df_year[df_year['merchant_index_spend'] >= top_threshold]
    range_top = top_df['distance_km'].max()

    for _, row in top_df.iterrows():
        ax_scatter.text(
            row['distance_km'],
            row['merchant_index_spend'],
            row['cardholder_location'],
            fontsize=8,
            alpha=0.7
        )

    ax_scatter.set_title(f"Distance vs. Spend - Q1 {year}")
    ax_scatter.set_xlabel("Distance (km)")
    ax_scatter.set_ylabel("Merchant Index Spend")

    ax_scatter.text(
        ax_scatter.get_xlim()[1],
        ax_scatter.get_ylim()[0]*3.5,
        s=f"Red dashed line = average ({round(avg_val, 2)})\nTop {top_pc}% labelled",
        size=8,
        ha='right'
    )

    ax_scatter.text(
        ax_scatter.get_xlim()[1] * .95,
        ax_scatter.get_ylim()[1] * .95,
        s=f"Top {top_pc}% range = {round(range_top,2)}km\n# areas top {top_pc}% = {len(top_df)}\nTotal areas = {len(df_year)}",
        ha='right',
        va='top'
    )

    ### RIGHT: MAP PLOT ###
    gdf = yearly_combined[year]

    gdf.plot(
        ax=ax_map,
        column='merchant_index_spend',
        legend=True,
        legend_kwds={'label': f'Merchant Index Spend'},
        alpha=0.5,
        edgecolor='black',
        vmin=global_vmin,
        vmax=global_vmax
    )

    for _, row in gdf.iterrows():
        x = row['centroid'].x
        y = row['centroid'].y
        ax_map.text(x, y, row['district'], fontsize=8, ha='center', va='center')

    cx.add_basemap(ax_map, crs=gdf.crs, source=cx.providers.OpenStreetMap.Mapnik)
    ax_map.set_title(f"Top {top_pc}% Districts Map - Q1 {year}", fontsize=12)
    ax_map.set_axis_off()

    # range_top_map = gdf['distance_km'].max()
    # ax_map.text(
    #     ax_map.get_xlim()[1],
    #     ax_map.get_ylim()[0],
    #     s=f"Top {top_pc}% range = {round(range_top_map,2)}km",
    #     ha='right',
    #     va='top'
    # )

plt.tight_layout()
plt.show()

fig.savefig(os.path.join('..','outputs','session3','spending_n17.png'), dpi=600)
```


Below we compare spending in all areas in London that have a Premier League Stadium so that we can compare with N17. 

To do this we find use the locations of stadia to identify their postcodes, then calculate the distance of each spending postcode from the stadium postcode as we did above. Then we plot over time and stadia.

```{python}
with open(os.path.join('..','data','stadium_locations.pkl'), "rb") as input_file:
    stadia = pickle.load(input_file)

msoas_of_interest = stadia['msoa21cd'].unique().tolist()
formatted_list = ', '.join([f"'{district}'" for district in msoas_of_interest])

with engine.connect() as con:
    query = f'''
            SELECT DISTINCT foo.pcds, foo.msoa21cd, foo.ladnm, loo.rgn21nm
            FROM pcode_census21_lookup foo
            LEFT JOIN lad21_lookup loo
            ON foo.lad21cd = loo.lad21cd
            WHERE foo.msoa21cd IN ({formatted_list})
            '''

    pcode_lookup = pd.read_sql(con=con, sql=query)

# Get the first part for pcode district
pcode_lookup['pcode_district'] = pcode_lookup['pcds'].str.split(' ').str[0]

pcode_lookup2 = pcode_lookup.drop_duplicates('pcode_district')
# Merge with stadia df to get stadium
pcode_lookup2 = pcode_lookup2.merge(stadia[['Stadium','msoa21cd']], how='left', on='msoa21cd')

# Get unique pcode districts to subset card data
pcode_districts = pcode_lookup['pcode_district'].unique().tolist()
card_data3 = card_data.loc[card_data['merchant_location'].isin(pcode_districts)]

# Merge with pcode lookup to get useful geographic references
card_data3 = card_data3.merge(pcode_lookup2[['pcode_district','ladnm','rgn21nm','Stadium']], how='left', left_on='merchant_location',right_on='pcode_district')

# Calculate the median for each year for each cardholder-merchant location (with other areas appended)
card_data3_median = card_data3.groupby(['year','cardholder_location','merchant_location','ladnm','rgn21nm','Stadium'], as_index=False)['merchant_index_spend'].median()


# Calculate distances
unique_combinations = card_data3_median.drop_duplicates(subset=['cardholder_location','merchant_location'])[['cardholder_location','merchant_location']]

unique_combinations_list = np.unique(unique_combinations.values.flatten()).tolist()

formatted_list = ', '.join([f"'{district}'" for district in unique_combinations_list])
with engine.connect() as con:
    query2 = f'''
                SELECT * 
                FROM postcode_district_boundaries
                WHERE district IN ({formatted_list})
                '''
    geoms_of_interest = gpd.read_postgis(query2, con=con, geom_col='geometry')

geoms_of_interest.to_pickle(os.path.join('..','data','geoms_of_interest3.pkl'))


# Get centroids
geoms_of_interest['centroid'] = geoms_of_interest.geometry.centroid
# Convert centroids to wgs84 in two steps using geoseries (this is becuase geodesic needs wgs84)
centroids_gs = gpd.GeoSeries(geoms_of_interest['centroid'], crs=geoms_of_interest.crs)
centroids_wgs84 = centroids_gs.to_crs(epsg=4326)
# Add the new crs centroids back into geoms_of_interest
geoms_of_interest['centroid'] = centroids_wgs84

# Step 1: Join geometries for cardholders and merchants
df = unique_combinations.merge(geoms_of_interest[['district','centroid']].rename(columns={
    'district': 'cardholder_location',
    'centroid': 'cardholder_centroid'
}), on='cardholder_location', how='left')

df = df.merge(geoms_of_interest[['district','centroid']].rename(columns={
    'district': 'merchant_location',
    'centroid': 'merchant_centroid'
}), on='merchant_location', how='left')

from geopy.distance import geodesic

# Calculate the distance using a function
def calc_distance(row):
    ch_centroid = row['cardholder_centroid']
    m_centroid = row['merchant_centroid']

    if ch_centroid is not None and m_centroid is not None:
        # Make sure they are shapely points
        if hasattr(ch_centroid, 'x') and hasattr(m_centroid, 'x'):
            point1 = (ch_centroid.y, ch_centroid.x)  # (lat, lon)
            point2 = (m_centroid.y, m_centroid.x)
            return geodesic(point1, point2).kilometers
    return None

df['distance_km'] = df.apply(calc_distance, axis=1)

all_stadia_df = df.merge(card_data3_median, how='right',on=['merchant_location','cardholder_location'])
```

Plot lines per year, plots per stadium

```{python}

subset = all_stadia_df.loc[(all_stadia_df['rgn21nm']=='London')]
subset['distance_bin'] = pd.cut(subset['distance_km'], bins=np.arange(0, 51, 1))

stadiums = subset['Stadium'].unique()
ncols = 2
nrows = int(np.ceil(len(stadiums) / ncols))

fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14, 5 * nrows), sharex=True, sharey=True)

for i, stadium in enumerate(stadiums):
    ax = axes[i // ncols, i % ncols]
    stadium_data = subset[subset['Stadium'] == stadium]

    for year, year_data in stadium_data.groupby('year'):
        binned = year_data.groupby('distance_bin', observed=True)['merchant_index_spend'].mean()
        binned.index = binned.index.map(lambda x: x.mid)
        ax.plot(binned.index, binned.values, label=f'{year}')

    ax.set_title(stadium)
    ax.set_xlabel('Distance (km)')
    ax.set_ylabel('Avg Spend')
    ax.grid(True)
    ax.legend(title='Year')

plt.tight_layout()
plt.show()

```

Plot lines per stadium, plots per year with Tottenham highlighted in red (this was what was used in session 3)

```{python}
# Binned, tott highlighted, lines per stadium, plots per year
subset = all_stadia_df.loc[(all_stadia_df['rgn21nm']=='London')]
subset['distance_bin'] = pd.cut(subset['distance_km'], bins=np.arange(0, 51, 2))
years = subset['year'].unique()
ncols = 2
nrows = int(np.ceil(len(years) / ncols))
top_pc = 5

fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14, 5 * nrows), sharex=False, sharey=True)

for i, year in enumerate(sorted(years)):
    ax = axes[i // ncols, i % ncols]
    year_data = subset[subset['year'] == year]

    
    for stadium, stadium_data in year_data.groupby('Stadium'):

        binned = stadium_data.groupby('distance_bin', observed=True)['merchant_index_spend'].median()
        binned.index = binned.index.map(lambda x: x.mid)
         # Conditional color: black for Tottenham, gray for others
        colour = 'red' if stadium == 'Tottenham Hotspur Stadium' else 'gray'
        ax.plot(binned.index, binned.values, color=colour)

        # Get the top x% and plot a vertical line
        top_threshold = np.quantile(stadium_data['merchant_index_spend'], 1 - (top_pc / 100))
        top_df = stadium_data[stadium_data['merchant_index_spend'] >= top_threshold]
        range_top = top_df['distance_km'].median()
        range_75p = top_df['distance_km'].quantile(.75)
        # ax.vlines(x=range_top, ymin=ax.get_ylim()[0], ymax=ax.get_ylim()[1], linestyles='dashed', color=colour)
        ax.axvline(x=range_top, linestyle='dashed', color=colour)
        # ax.axvline(x=range_75p, linestyle='dashed', color=colour)
        if stadium == 'Tottenham Hotspur Stadium':
            nudge_x = (ax.get_xlim()[1] - ax.get_xlim()[0]) * .02
            ax.text(x=range_top + nudge_x, y=6, s=f'Average distance for top {top_pc}%: {round(range_top,0)}km',color='red', fontsize=12)

    ax.set_title(f'{year}', fontsize=14)
    ax.set_xlabel('Distance (km)')
    ax.set_ylabel('Average proportion of spend')
    ax.tick_params(labelleft=True)
    ax.grid(True)

# Hide any unused subplots
total_axes = nrows * ncols
for j in range(len(years), total_axes):
    fig.delaxes(axes[j // ncols, j % ncols])

fig.suptitle('Spending in stadium locations by postcode district', fontsize=20)
fig.text(x=0.5 + 1/50, y=0 + 1/25, va='bottom',ha='left',s=f'Notes:\n1. Only showing areas up to 50km away\n2. Red solid line indicates Tottenham Hotspur Stadium\n3. Grey solid lines indicate other London stadia\n4. Red dashed line indicates average distance for the top {top_pc}%of cardholder\nlocations spending at Tottenham Hotspur Stadium\n5. Grey dashed lines indicate average distance for the top {top_pc}%of cardholder\nlocations spending at other London stadia', fontsize=12)
plt.tight_layout()
plt.show()

fig.savefig(os.path.join('..','outputs','session3','business_spending.png'), dpi=600)

```

Plot each of the above as a seperate plot (this was used in session 3)

```{python}
subset = all_stadia_df.loc[(all_stadia_df['rgn21nm'] == 'London')]
subset['distance_bin'] = pd.cut(subset['distance_km'], bins=np.arange(0, 51, 2))
years = sorted(subset['year'].unique())
top_pc = 5

# --- Get global axis limits ---
# X-axis range (midpoints of distance bins)
x_vals = subset['distance_bin'].dropna().unique().categories.mid
x_min, x_max = x_vals.min(), x_vals.max()

# Y-axis range (all median merchant_index_spend values)
y_vals = (
    subset.groupby(['year', 'Stadium', 'distance_bin'], observed=True)['merchant_index_spend']
    .median()
    .dropna()
)
y_min, y_max = y_vals.min(), y_vals.max()

for year in years:
    year_data = subset[subset['year'] == year]
    fig, ax = plt.subplots(figsize=(10, 6))
    
    for stadium, stadium_data in year_data.groupby('Stadium'):
        binned = stadium_data.groupby('distance_bin', observed=True)['merchant_index_spend'].median()
        binned.index = binned.index.map(lambda x: x.mid)
        colour = 'red' if stadium == 'Tottenham Hotspur Stadium' else 'gray'
        ax.plot(binned.index, binned.values, color=colour, label=stadium if stadium == 'Tottenham Hotspur Stadium' else None)
        
        # Top % line
        top_threshold = np.quantile(stadium_data['merchant_index_spend'], 1 - (top_pc / 100))
        top_df = stadium_data[stadium_data['merchant_index_spend'] >= top_threshold]
        range_top = top_df['distance_km'].median()
        ax.axvline(x=range_top, linestyle='dashed', color=colour)
        
        if stadium == 'Tottenham Hotspur Stadium':
            nudge_x = (x_max - x_min) * 0.02
            ax.text(x=range_top + nudge_x, y=6, s=f'Avg dist top {top_pc}%: {round(range_top,0)}km',
                    color='red', fontsize=10)

    ax.set_xlim(x_min-1, x_max)
    ax.set_ylim(y_min, y_max+1)
    ax.set_title(f'Spending by Distance – {year}', fontsize=14)
    ax.set_xlabel('Distance (km)')
    ax.set_ylabel('Avg. proportion of spend')
    ax.grid(True)
    ax.legend(loc='upper right')
    
    plt.tight_layout()
    plt.savefig(os.path.join('..', 'outputs', 'session3', f'business_spending_{year}.png'), dpi=600)
    plt.show()

```
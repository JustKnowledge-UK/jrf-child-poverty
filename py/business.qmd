---
title: "Untitled"
format: html
---

```{python}
import pandas as pd
import requests
from bs4 import BeautifulSoup
import re
import geopandas as gpd
from shapely.geometry import Point
import matplotlib.pyplot as plt
import contextily as cx
from adjustText import adjust_text
import configparser
import psycopg2
import os
import pickle
import numpy as np
from sklearn.neighbors import BallTree, radius_neighbors_graph
from scipy.spatial import cKDTree
import janitor
import zipfile
import glob

config = configparser.ConfigParser()
config.read(os.path.join('..', 'db_config.ini'))

db_params = dict(config['postgresql'])
```

```{python}
with psycopg2.connect(**db_params) as con:
    query = '''
            SELECT DISTINCT pcds, ladnm 
            FROM pcode_census21_lookup
            WHERE ladnm IN ('Haringey','Hackney')
            '''

    pcode_lookup = pd.read_sql(con=con, sql=query)

# Get the first part for pcode district
pcode_lookup['pcode_district'] = pcode_lookup['pcds'].str.split(' ').str[0]

pcode_lookup.to_pickle(os.path.join('..','data','pcode_lookup.pkl'))
haringey_pcode_districts = pcode_lookup.loc[pcode_lookup['ladnm']=='Haringey','pcode_district'].unique().tolist()

pcodes_of_interest = pcode_lookup['pcode_district'].unique().tolist()

# Safely convert list to SQL-safe string
formatted_list = ', '.join([f"'{district}'" for district in haringey_pcode_districts])
# with psycopg2.connect(**db_params) as con:
query2 = f'''
            SELECT * 
            FROM postcode_district_boundaries
            WHERE district IN ({formatted_list})
            '''
haringey_districts_gpd = gpd.read_postgis(query2, con=con, geom_col='geometry')

haringey_districts_gpd.to_pickle(os.path.join('..','data','pcode_districts_haringey.pkl'))

```

```{python}
url = 'https://www.nomisweb.co.uk/output/eop/postal_district_quarterly_indexed_map_data.zip'

# Check if it's already downloaded
basename = os.path.basename(url)
path = os.path.join('..', 'data', basename)
if os.path.isfile(path)==False:
    req = requests.get(url)
    with open(path, 'wb') as output_file:
        output_file.write(req.content)
else:
    print('Data already acquired. Loading it')

# if it's a zipped folder, unzip
# define outpath as same as in path minus .zip
out_path = os.path.splitext(path)[0] # This removes the .zip extension 

# Create the extraction directory if it doesn't exist
if not os.path.exists(out_path):
    os.makedirs(out_path)

# unzip
with zipfile.ZipFile(path, 'r') as zip_ref:
    zip_ref.extractall(out_path)

# Use glob to get all CSV files in the directory
files = glob.glob(os.path.join(out_path, '*.csv'))

card_data = pd.read_csv(files[0])

# Subset to card data that involves postcodes of interest
card_data2 = card_data.loc[card_data['cardholder_location'].isin(pcodes_of_interest) | card_data['merchant_location'].isin(pcodes_of_interest)]

# Split time period into year and quarter
card_data2[['year','quarter']] = card_data2['time_period_value'].str.split('Q', expand=True).astype(int)

month_map = {1: '-03-01', 2: '-06-01', 3: '-09-01', 4: '-12-01'}

card_data2['year_month'] = pd.to_datetime(
    card_data2['year'].astype(str) + card_data2['quarter'].map(month_map)
).dt.strftime('%Y-%m')



```

# Test plot to visualise districts

These seem like an OK size for use and differentiate areas in Haringey reasonably well

```{python}
haringey_districts_gpd['centroid'] = haringey_districts_gpd.geometry.centroid

fig, ax = plt.subplots(1,1, figsize = [8,8])

haringey_districts_gpd.plot(ax=ax, alpha=0.5, edgecolor='black')
# Add labels at centroids
for idx, row in haringey_districts_gpd.iterrows():
    x = row['centroid'].x
    y = row['centroid'].y
    label = row['district']  
    ax.text(x, y, label, fontsize=8, ha='center', va='center', color='white')
cx.add_basemap(ax, crs = haringey_districts_gpd.crs, source=cx.providers.OpenStreetMap.Mapnik)
```

# Card spending

cardholder_index_spend: Proportion of total spend at merchants in the merchant location by cardholders based in the cardholder location.

This translates to: "Of all the money spent at merchants in a specific location (Merchant_location), how much came from cardholders who live in a specific location (Cardholder_location)?"

merchant_index_spend: Proportion of total spend by cardholders based in the cardholder location at merchants based in the merchant location.

This translates to: "Of all the money spent by cardholders from a specific location (Cardholder_location), how much was spent at merchants in a specific location (Merchant_location)?"

NOTE I think the above are the wrong way round

2019Q1 adds to 100. So merchant index for a postcode in 2019Q1 is describing the proportion of spending at merchants in that postcode by cardholders in cardholder locations. Then subsequent quarters are describing the proprotion of spendign at merchants in that postcdoe adn that quarter by cardholders in cardholder location, divided by the total spending at merchants in that postcode in 2019Q1.

## Spending at Northumberland Park

```{python}
n17_merchant = card_data2.loc[card_data2['merchant_location'] == 'N17']

# plot to explore
fig, ax = plt.subplots(figsize = [8,8])
# Create the scatter plot
ax.scatter(x=n17_merchant['year_month'], y=n17_merchant['cardholder_index_spend'])
```

The proportions don't add up in the way you'd expect, but could this be to do with redaction etc.?

```{python}
# First check that proportions add to 100 as expected
test = card_data.groupby(['time_period_value'])['cardholder_index_spend'].sum()
# assert (test.round(6) == 1).all()  # Should be True if all sums â‰ˆ 1.0

test2 = card_data.groupby(['time_period_value','cardholder_location'])['merchant_index_spend'].sum().reset_index()

test3 = card_data.groupby(['time_period_value','merchant_location'])['cardholder_index_spend'].sum().reset_index()

# 2019s add to 100, but not as the documentation describes. I think the documentation is just wrong
q1_2019 = card_data.loc[card_data['time_period_value']=='2019Q1']
test4 = q1_2019.groupby(['time_period_value','merchant_location'])['merchant_index_spend'].sum().reset_index()

test5 = q1_2019.groupby(['time_period_value','cardholder_location'])['cardholder_index_spend'].sum().reset_index()

test6 = card_data.groupby(['time_period_value','merchant_location'])['merchant_index_spend'].sum().reset_index()
```

Maybe we want to look at spending by distacne

```{python}
unique_combinations = n17_merchant.drop_duplicates(subset=['cardholder_location','merchant_location'])[['cardholder_location','merchant_location']]

unique_combinations_list = np.unique(unique_combinations.values.flatten()).tolist()

formatted_list = ', '.join([f"'{district}'" for district in unique_combinations_list])

with psycopg2.connect(**db_params) as con:
    query2 = f'''
             SELECT * 
             FROM postcode_district_boundaries
             WHERE district IN ({formatted_list})
             '''
    geoms_of_interest = gpd.read_postgis(query2, con=con, geom_col='geometry')

geoms_of_interest.to_pickle(os.path.join('..','data','geoms_of_interest1.pkl'))


# Get centroids
geoms_of_interest['centroid'] = geoms_of_interest.geometry.centroid
# Convert centroids to wgs84 in two steps using geoseries (this is becuase geodesic needs wgs84)
centroids_gs = gpd.GeoSeries(geoms_of_interest['centroid'], crs=geoms_of_interest.crs)
centroids_wgs84 = centroids_gs.to_crs(epsg=4326)
# Add the new crs centroids back into geoms_of_interest
geoms_of_interest['centroid'] = centroids_wgs84

# Step 1: Join geometries for cardholders and merchants
df = unique_combinations.merge(geoms_of_interest[['district','centroid']].rename(columns={
    'district': 'cardholder_location',
    'centroid': 'cardholder_centroid'
}), on='cardholder_location', how='left')

df = df.merge(geoms_of_interest[['district','centroid']].rename(columns={
    'district': 'merchant_location',
    'centroid': 'merchant_centroid'
}), on='merchant_location', how='left')

from geopy.distance import geodesic

# Calculate the distance using a function
def calc_distance(row):
    ch_centroid = row['cardholder_centroid']
    m_centroid = row['merchant_centroid']

    if ch_centroid is not None and m_centroid is not None:
        # Make sure they are shapely points
        if hasattr(ch_centroid, 'x') and hasattr(m_centroid, 'x'):
            point1 = (ch_centroid.y, ch_centroid.x)  # (lat, lon)
            point2 = (m_centroid.y, m_centroid.x)
            return geodesic(point1, point2).kilometers
    return None

df['distance_km'] = df.apply(calc_distance, axis=1)

```

```{python}
df2 = df.merge(n17_merchant, how='right',on=['merchant_location','cardholder_location'])

df2_2023 = df2.loc[df2['year']==2023]
df2_2023['ch_avg_2023'] = df2_2023.groupby('cardholder_location')['cardholder_index_spend'].transform('mean')
df2_2023 = df2_2023.drop_duplicates(subset=['cardholder_location'])

top_pc = 1
avg_2023 = np.mean(df2_2023['ch_avg_2023'])
top_1pc = np.quantile(df2_2023['ch_avg_2023'], q=[1-(top_pc/100)])[0]

# plot to explore
fig, ax = plt.subplots(figsize = [8,8])
# Create the scatter plot
ax.scatter(x=df2_2023['distance_km'], y=df2_2023['ch_avg_2023'])

ax.set_xlabel('Distance (km)')
ax.set_ylabel('Average Proportion of Cardholder Spend in N17 (2023)')
ax.set_title('Scatter Plot of Distance vs. Average Cardholder Spend');

# Add labels where ch_avg_2023 > 1
texts = []
for _, row in df2_2023[df2_2023['ch_avg_2023'] > top_1pc].iterrows():
    texts.append(ax.text(row['distance_km'], row['ch_avg_2023'], row['cardholder_location'], fontsize=8, alpha=0.7))

# Automatically adjust positions to prevent overlap
adjust_text(texts, ax=ax, arrowprops=dict(arrowstyle='-', color='gray', lw=0.5))

plt.axhline(y=avg_2023, color='r', linestyle='dashed')
ax.text(
    ax.get_xlim()[1],
    ax.get_ylim()[0] * 4,
    s='Dashed red line indicates the average proportion for 2023 (' + str(round(avg_2023, 2)) + ')'\
        '\nLabelled areas are the top ' + str(top_pc) + '% of cardholder postcodes',
    size=8,
    ha='right'
);

```

Exploring the areas with above 1% spend

```{python}
top_pc_districts = df2_2023.loc[df2_2023['ch_avg_2023'] > top_1pc, 'cardholder_location'].tolist()
# Safely convert list to SQL-safe string
formatted_list = ', '.join([f"'{district}'" for district in top_pc_districts])
with psycopg2.connect(**db_params) as con:
    query2 = f'''
             SELECT * 
             FROM postcode_district_boundaries
             WHERE district IN ({formatted_list})
             '''
    top_pc_gdf = gpd.read_postgis(query2, con=con, geom_col='geometry')

top_pc_gdf.to_pickle(os.path.join('..','data','top_pc_gdf.pkl'))

top_pc_gdf['centroid'] = top_pc_gdf.geometry.centroid

```

```{python}
fig, ax = plt.subplots(1,1, figsize = [8,8])

top_pc_gdf.plot(ax=ax, alpha=0.5, edgecolor='black')
# Add labels at centroids
for idx, row in top_pc_gdf.iterrows():
    x = row['centroid'].x
    y = row['centroid'].y
    label = row['district']  
    ax.text(x, y, label, fontsize=8, ha='center', va='center', color='black')
cx.add_basemap(ax, crs = top_pc_gdf.crs, source=cx.providers.OpenStreetMap.Mapnik)
ax.set_axis_off()
```

```{python}
combined1 = top_pc_gdf.merge(df2_2023,how='left',left_on='district',right_on='cardholder_location')

fig, ax = plt.subplots(1,1, figsize = [8,8])

combined1.plot(ax=ax, column='ch_avg_2023', legend=True, legend_kwds={'label': 'Average proprotion of spending in N17'}, alpha=0.5, edgecolor='black')
# Add labels at centroids
for idx, row in combined1.iterrows():
    x = row['centroid'].x
    y = row['centroid'].y
    label = row['district']  
    ax.text(x, y, label, fontsize=8, ha='center', va='center', color='black')
cx.add_basemap(ax, crs = combined1.crs, source=cx.providers.OpenStreetMap.Mapnik)
ax.set_axis_off()
```

Focusing on areas < 100km away

```{python}
top_pc_districts2 = df2_2023.loc[(df2_2023['ch_avg_2023'] > top_1pc) & (df2_2023['distance_km'] < 100), 'cardholder_location'].tolist()
# Safely convert list to SQL-safe string
formatted_list = ', '.join([f"'{district}'" for district in top_pc_districts2])
with psycopg2.connect(**db_params) as con:
    query2 = f'''
             SELECT * 
             FROM postcode_district_boundaries
             WHERE district IN ({formatted_list})
             '''
    top_pc_gdf2 = gpd.read_postgis(query2, con=con, geom_col='geometry')

top_pc_gdf2.to_pickle(os.path.join('..','data','top_pc_gdf2.pkl'))

top_pc_gdf2['centroid'] = top_pc_gdf2.geometry.centroid

fig, ax = plt.subplots(1,1, figsize = [8,8])

top_pc_gdf2.plot(ax=ax, alpha=0.5, edgecolor='black')
# Add labels at centroids
for idx, row in top_pc_gdf2.iterrows():
    x = row['centroid'].x
    y = row['centroid'].y
    label = row['district']  
    ax.text(x, y, label, fontsize=8, ha='center', va='center', color='black')
cx.add_basemap(ax, crs = top_pc_gdf2.crs, source=cx.providers.OpenStreetMap.Mapnik)
ax.set_axis_off()
```

```{python}
combined2 = top_pc_gdf2.merge(df2_2023,how='left',left_on='district',right_on='cardholder_location')

fig, ax = plt.subplots(1,1, figsize = [8,8])

combined2.plot(ax=ax, column='ch_avg_2023', legend=True, legend_kwds={'label': 'Average proprotion of spending in N17'}, alpha=0.5, edgecolor='black')
# Add labels at centroids
for idx, row in combined2.iterrows():
    x = row['centroid'].x
    y = row['centroid'].y
    label = row['district']  
    ax.text(x, y, label, fontsize=8, ha='center', va='center', color='black')
cx.add_basemap(ax, crs = combined2.crs, source=cx.providers.OpenStreetMap.Mapnik)
ax.set_axis_off()
```

Focusing on areas in London

```{python}
top_pc_districts3 = df2_2023.loc[(df2_2023['ch_avg_2023'] > top_1pc) & (df2_2023['distance_km'] < 20), 'cardholder_location'].tolist()
# Safely convert list to SQL-safe string
formatted_list = ', '.join([f"'{district}'" for district in top_pc_districts3])
with psycopg2.connect(**db_params) as con:
    query2 = f'''
             SELECT * 
             FROM postcode_district_boundaries
             WHERE district IN ({formatted_list})
             '''
    top_pc_gdf3 = gpd.read_postgis(query2, con=con, geom_col='geometry')

top_pc_gdf3.to_pickle(os.path.join('..','data','top_pc_gdf3.pkl'))

top_pc_gdf3['centroid'] = top_pc_gdf3.geometry.centroid

fig, ax = plt.subplots(1,1, figsize = [8,8])

top_pc_gdf3.plot(ax=ax, alpha=0.5, edgecolor='black')
# Add labels at centroids
for idx, row in top_pc_gdf3.iterrows():
    x = row['centroid'].x
    y = row['centroid'].y
    label = row['district']  
    ax.text(x, y, label, fontsize=8, ha='center', va='center', color='black')
cx.add_basemap(ax, crs = top_pc_gdf3.crs, source=cx.providers.OpenStreetMap.Mapnik)
ax.set_axis_off()
```

Combine with data to plot choropleth

```{python}
combined3 = top_pc_gdf3.merge(df2_2023, how='left',left_on='district',right_on='cardholder_location')

fig, ax = plt.subplots(1,1, figsize = [8,8])

combined3.plot(ax=ax, column='ch_avg_2023', legend=True, legend_kwds={'label': 'Average proprotion of spending in N17'}, alpha=0.5, edgecolor='black')
# Add labels at centroids
for idx, row in combined3.iterrows():
    x = row['centroid'].x
    y = row['centroid'].y
    label = row['district']  
    ax.text(x, y, label, fontsize=8, ha='center', va='center', color='black')
cx.add_basemap(ax, crs = combined3.crs, source=cx.providers.OpenStreetMap.Mapnik)
ax.set_axis_off()
```

All areas

```{python}
all_districts = df2_2023['cardholder_location'].drop_duplicates().tolist()
# Safely convert list to SQL-safe string
formatted_list = ', '.join([f"'{district}'" for district in all_districts])
with psycopg2.connect(**db_params) as con:
    query2 = f'''
             SELECT * 
             FROM postcode_district_boundaries
             WHERE district IN ({formatted_list})
             '''
    all_districts_gdf = gpd.read_postgis(query2, con=con, geom_col='geometry')

all_districts_gdf.to_pickle(os.path.join('..','data','top_pc_gdf.pkl'))

all_districts_gdf['centroid'] = all_districts_gdf.geometry.centroid

```

```{python}
combined4 = all_districts_gdf.merge(df2_2023,how='left',left_on='district',right_on='cardholder_location')

fig, ax = plt.subplots(1,1, figsize = [8,8])

combined4.plot(ax=ax, column='ch_avg_2023', legend=True, legend_kwds={'label': 'Average proportion of spending in N17'}, alpha=0.5, edgecolor='black')
# Add labels at centroids
# for idx, row in combined4.iterrows():
#     x = row['centroid'].x
#     y = row['centroid'].y
#     label = row['district']  
#     ax.text(x, y, label, fontsize=8, ha='center', va='center', color='black')
cx.add_basemap(ax, crs = combined4.crs, source=cx.providers.OpenStreetMap.Mapnik)
ax.set_axis_off()
```


# Over time

We could take the median for the whole year perhaps...

```{python}
from adjustText import adjust_text

# Ensure years and top percentage are set
years = sorted(df2['year'].unique())
top_pc = 5

# Setup one subplot per year
n_years = len(years)
fig, axes = plt.subplots(n_years, 1, figsize=(8, 5 * n_years), sharex=True)

if n_years == 1:
    axes = [axes]  # make iterable if only one plot

for ax, year in zip(axes, years):
    # Filter for current year and Q1
    df_year = df2[(df2['year'] == year) & (df2['quarter'] == 1)].copy()
    
    # Calculate average and top X% threshold
    avg_val = df_year['merchant_index_spend'].mean()

    # Scatter plot
    ax.scatter(df_year['distance_km'], df_year['merchant_index_spend'], alpha=0.6)

    # Dashed horizontal line at average
    ax.axhline(y=avg_val, color='r', linestyle='dashed')

    # Label points above top threshold
    texts = []
    top_threshold = np.quantile(df_year['merchant_index_spend'], 1 - (top_pc / 100))
    top_df = df_year[df_year['merchant_index_spend'] >= top_threshold]
    range_top = top_df['distance_km'].max()
    for _, row in top_df.iterrows():
        ax.text(
            row['distance_km'], 
            row['merchant_index_spend'], 
            row['cardholder_location'], 
            fontsize=8, 
            alpha=0.7
        )
    #     texts.append(ax.text(
    #         row['distance_km'], 
    #         row['merchant_index_spend'], 
    #         row['cardholder_location'], 
    #         fontsize=8, 
    #         alpha=0.7
    #     ))

    # adjust_text(texts, ax=ax, arrowprops=dict(arrowstyle='-', color='gray', lw=0.5))

    # Labels and annotations
    ax.set_title(f"Distance vs. Merchant Index Spend in N17 - Q1 {year}")
    ax.set_ylabel(f"Merchant Index Spend ({year})")
    ax.set_xlabel("Distance (km)")

    ax.text(
        ax.get_xlim()[1],
        ax.get_ylim()[0]*3.5,
        s=f"Red dashed line = average ({round(avg_val, 2)})\nTop {top_pc}% labelled",
        size=8,
        ha='right'
    )

    
    ax.text(
        ax.get_xlim()[1] * .95,
        ax.get_ylim()[1] * .95,
        s=f"Top {top_pc}% range = {round(range_top,2)}km\nNumber areas top {top_pc}% = {len(top_df)}\nTotal areas = {len(df_year)}",
        ha='right',
        va='top'
    )

plt.tight_layout()
plt.show()

fig.savefig(os.path.join('..','outputs','session3','scatter_spending_n17.png'), dpi=600)
```

```{python}
years = sorted(df2['year'].unique().tolist())
top_pc = 5
yearly_combined = {}

with psycopg2.connect(**db_params) as con:
    for year in years:
        df_year = df2[(df2['year'] == year) & (df2['quarter']==1)].copy()
        df_year = df_year.drop_duplicates(subset=['cardholder_location'])

        top_threshold = np.quantile(df_year['merchant_index_spend'], 1 - (top_pc / 100))
        top_districts = df_year[df_year['merchant_index_spend'] >= top_threshold]['cardholder_location'].tolist()
        formatted_list = ', '.join([f"'{district}'" for district in top_districts])

        query = f'''
            SELECT * 
            FROM postcode_district_boundaries
            WHERE district IN ({formatted_list})
        '''
        gdf = gpd.read_postgis(query, con=con, geom_col='geometry')
        gdf['centroid'] = gdf.geometry.centroid

        combined = gdf.merge(df_year, how='left', left_on='district', right_on='cardholder_location')
        yearly_combined[year] = combined

output_dir = os.path.join('..','data')
# Pickle the dictionary
with open(os.path.join(output_dir, 'yearly_combined.pkl'), 'wb') as f:
    pickle.dump(yearly_combined, f)

# Compute global min and max for 'merchant_index_spend'
all_spend_values = pd.concat([gdf['merchant_index_spend'] for gdf in yearly_combined.values()])
global_vmin = all_spend_values.min()
global_vmax = all_spend_values.max()


# Plotting faceted maps
n_years = len(years)
fig, axes = plt.subplots(n_years, 1, figsize=(10, 6 * n_years))

if n_years == 1:
    axes = [axes]

for ax, year in zip(axes, years):
    gdf = yearly_combined[year]
    gdf.plot(
        ax=ax, 
        column='merchant_index_spend', 
        legend=True, 
        legend_kwds={'label': f'Index of spending in N17 ({year} Q1)'},
        alpha=0.5, 
        edgecolor='black',
        vmin = global_vmin,
        vmax = global_vmax
    )

    # Add district labels
    for _, row in gdf.iterrows():
        x = row['centroid'].x
        y = row['centroid'].y
        ax.text(x, y, row['district'], fontsize=8, ha='center', va='center')

    cx.add_basemap(ax, crs=gdf.crs, source=cx.providers.OpenStreetMap.Mapnik)
    ax.set_title(f"Top {top_pc}% Districts - {year} Q1", fontsize=14)
    ax.set_axis_off()

    range_top = gdf['distance_km'].max()
    ax.text(
        ax.get_xlim()[1],
        ax.get_ylim()[0],
        s=f"Top {top_pc}% range = {round(range_top,2)}km",
        ha='right',
        va='top'
    )

plt.tight_layout()
plt.show()

fig.savefig(os.path.join('..','outputs','session3','choro_spending_n17.png'), dpi=600)
```

```{python}
import matplotlib.pyplot as plt
from adjustText import adjust_text
import numpy as np
import os
import pickle
import geopandas as gpd
import contextily as cx
import psycopg2

years = sorted(df2['year'].unique().tolist())
top_pc = 5
n_years = len(years)

# Prepare data for map plots (as in your second code block)
yearly_combined = {}
with psycopg2.connect(**db_params) as con:
    for year in years:
        df_year = df2[(df2['year'] == year) & (df2['quarter'] == 1)].copy()
        df_year = df_year.drop_duplicates(subset=['cardholder_location'])

        top_threshold = np.quantile(df_year['merchant_index_spend'], 1 - (top_pc / 100))
        top_districts = df_year[df_year['merchant_index_spend'] >= top_threshold]['cardholder_location'].tolist()
        formatted_list = ', '.join([f"'{district}'" for district in top_districts])

        query = f'''
            SELECT * 
            FROM postcode_district_boundaries
            WHERE district IN ({formatted_list})
        '''
        gdf = gpd.read_postgis(query, con=con, geom_col='geometry')
        gdf['centroid'] = gdf.geometry.centroid
        combined = gdf.merge(df_year, how='left', left_on='district', right_on='cardholder_location')
        yearly_combined[year] = combined

# Global color scale
all_spend_values = pd.concat([gdf['merchant_index_spend'] for gdf in yearly_combined.values()])
global_vmin = all_spend_values.min()
global_vmax = all_spend_values.max()

# Create 5x2 subplot
fig, axes = plt.subplots(nrows=n_years, ncols=2, figsize=(16, 6 * n_years))

# Ensure axes is always 2D
if n_years == 1:
    axes = np.array([axes])

for i, year in enumerate(years):
    ax_scatter = axes[i, 0]
    ax_map = axes[i, 1]

    ### LEFT: SCATTER PLOT ###
    df_year = df2[(df2['year'] == year) & (df2['quarter'] == 1)].copy()
    avg_val = df_year['merchant_index_spend'].mean()

    ax_scatter.scatter(df_year['distance_km'], df_year['merchant_index_spend'], alpha=0.6)
    ax_scatter.axhline(y=avg_val, color='r', linestyle='dashed')

    top_threshold = np.quantile(df_year['merchant_index_spend'], 1 - (top_pc / 100))
    top_df = df_year[df_year['merchant_index_spend'] >= top_threshold]
    range_top = top_df['distance_km'].max()

    for _, row in top_df.iterrows():
        ax_scatter.text(
            row['distance_km'],
            row['merchant_index_spend'],
            row['cardholder_location'],
            fontsize=8,
            alpha=0.7
        )

    ax_scatter.set_title(f"Distance vs. Spend - Q1 {year}")
    ax_scatter.set_xlabel("Distance (km)")
    ax_scatter.set_ylabel("Merchant Index Spend")

    ax_scatter.text(
        ax_scatter.get_xlim()[1],
        ax_scatter.get_ylim()[0]*3.5,
        s=f"Red dashed line = average ({round(avg_val, 2)})\nTop {top_pc}% labelled",
        size=8,
        ha='right'
    )

    ax_scatter.text(
        ax_scatter.get_xlim()[1] * .95,
        ax_scatter.get_ylim()[1] * .95,
        s=f"Top {top_pc}% range = {round(range_top,2)}km\n# areas top {top_pc}% = {len(top_df)}\nTotal areas = {len(df_year)}",
        ha='right',
        va='top'
    )

    ### RIGHT: MAP PLOT ###
    gdf = yearly_combined[year]

    gdf.plot(
        ax=ax_map,
        column='merchant_index_spend',
        legend=True,
        legend_kwds={'label': f'Index of Spending in N17 (Q1 {year})'},
        alpha=0.5,
        edgecolor='black',
        vmin=global_vmin,
        vmax=global_vmax
    )

    for _, row in gdf.iterrows():
        x = row['centroid'].x
        y = row['centroid'].y
        ax_map.text(x, y, row['district'], fontsize=8, ha='center', va='center')

    cx.add_basemap(ax_map, crs=gdf.crs, source=cx.providers.OpenStreetMap.Mapnik)
    ax_map.set_title(f"Top {top_pc}% Districts Map - Q1 {year}", fontsize=12)
    ax_map.set_axis_off()

    range_top_map = gdf['distance_km'].max()
    ax_map.text(
        ax_map.get_xlim()[1],
        ax_map.get_ylim()[0],
        s=f"Top {top_pc}% range = {round(range_top_map,2)}km",
        ha='right',
        va='top'
    )

plt.tight_layout()
plt.show()

```

# a different area

```{python}
e8_merchant = card_data2.loc[card_data2['merchant_location'] == 'E8']
unique_combinations = e8_merchant.drop_duplicates(subset=['cardholder_location','merchant_location'])[['cardholder_location','merchant_location']]

unique_combinations_list = np.unique(unique_combinations.values.flatten()).tolist()

formatted_list = ', '.join([f"'{district}'" for district in unique_combinations_list])

with psycopg2.connect(**db_params) as con:
    query2 = f'''
             SELECT * 
             FROM postcode_district_boundaries
             WHERE district IN ({formatted_list})
             '''
    geoms_of_interest = gpd.read_postgis(query2, con=con, geom_col='geometry')

geoms_of_interest.to_pickle(os.path.join('..','data','geoms_of_interest2.pkl'))

# Get centroids
geoms_of_interest['centroid'] = geoms_of_interest.geometry.centroid
# Convert centroids to wgs84 in two steps using geoseries (this is becuase geodesic needs wgs84)
centroids_gs = gpd.GeoSeries(geoms_of_interest['centroid'], crs=geoms_of_interest.crs)
centroids_wgs84 = centroids_gs.to_crs(epsg=4326)
# Add the new crs centroids back into geoms_of_interest
geoms_of_interest['centroid'] = centroids_wgs84

# Step 1: Join geometries for cardholders and merchants
df = unique_combinations.merge(geoms_of_interest[['district','centroid']].rename(columns={
    'district': 'cardholder_location',
    'centroid': 'cardholder_centroid'
}), on='cardholder_location', how='left')

df = df.merge(geoms_of_interest[['district','centroid']].rename(columns={
    'district': 'merchant_location',
    'centroid': 'merchant_centroid'
}), on='merchant_location', how='left')

from geopy.distance import geodesic

# Calculate the distance using a function
def calc_distance(row):
    ch_centroid = row['cardholder_centroid']
    m_centroid = row['merchant_centroid']

    if ch_centroid is not None and m_centroid is not None:
        # Make sure they are shapely points
        if hasattr(ch_centroid, 'x') and hasattr(m_centroid, 'x'):
            point1 = (ch_centroid.y, ch_centroid.x)  # (lat, lon)
            point2 = (m_centroid.y, m_centroid.x)
            return geodesic(point1, point2).kilometers
    return None

df['distance_km'] = df.apply(calc_distance, axis=1)

df3 = df.merge(e8_merchant, how='right',on=['merchant_location','cardholder_location'])

# Ensure years and top percentage are set
years = sorted(df3['year'].unique())
top_pc = 5

# Setup one subplot per year
n_years = len(years)
fig, axes = plt.subplots(n_years, 1, figsize=(8, 5 * n_years), sharex=True)

if n_years == 1:
    axes = [axes]  # make iterable if only one plot

for ax, year in zip(axes, years):
    # Filter for current year and Q1
    df_year = df3[(df3['year'] == year) & (df3['quarter'] == 1)].copy()
    print(len(df_year))
    # Calculate average and top X% threshold
    avg_val = df_year['merchant_index_spend'].mean()

    # Scatter plot
    ax.scatter(df_year['distance_km'], df_year['merchant_index_spend'], alpha=0.6)

    # Dashed horizontal line at average
    ax.axhline(y=avg_val, color='r', linestyle='dashed')

    # Label points above top threshold
    texts = []
    top_threshold = np.quantile(df_year['merchant_index_spend'], 1 - (top_pc / 100))
    top_df = df_year[df_year['merchant_index_spend'] >= top_threshold]
    range_top = top_df['distance_km'].max()

    for _, row in top_df.iterrows():
        ax.text(
            row['distance_km'], 
            row['merchant_index_spend'], 
            row['cardholder_location'], 
            fontsize=8, 
            alpha=0.7
        )
    #     texts.append(ax.text(
    #         row['distance_km'], 
    #         row['merchant_index_spend'], 
    #         row['cardholder_location'], 
    #         fontsize=8, 
    #         alpha=0.7
    #     ))

    # adjust_text(texts, ax=ax, arrowprops=dict(arrowstyle='-', color='gray', lw=0.5))

    # Labels and annotations
    ax.set_title(f"Distance vs. Merchant Index Spend in E8 - Q1 {year}")
    ax.set_ylabel(f"Merchant Index Spend ({year})")
    ax.set_xlabel("Distance (km)")

    ax.text(
        ax.get_xlim()[1],
        ax.get_ylim()[0]*5,
        s=f"Red dashed line = average ({round(avg_val, 2)})\nTop {top_pc}% labelled",
        size=8,
        ha='right'
    )

    ax.text(
        ax.get_xlim()[1] * .95,
        ax.get_ylim()[1] * .95,
        s=f"Top {top_pc}% range = {round(range_top,2)}km\nNumber areas top {top_pc}% = {len(top_df)}\nTotal areas = {len(df_year)}",
        ha='right',
        va='top'
    )

plt.tight_layout()
plt.show()

fig.savefig(os.path.join('..','outputs','session3','scatter_spending_e8.png'), dpi=600)
```


```{python}

years = sorted(df3['year'].unique().tolist())
top_pc = 5
yearly_combined = {}

with psycopg2.connect(**db_params) as con:
    for year in years:
        df_year = df3[(df3['year'] == year) & (df3['quarter']==1)].copy()
        df_year = df_year.drop_duplicates(subset=['cardholder_location'])

        top_threshold = np.quantile(df_year['merchant_index_spend'], 1 - (top_pc / 100))
        top_districts = df_year[df_year['merchant_index_spend'] >= top_threshold]['cardholder_location'].tolist()
        formatted_list = ', '.join([f"'{district}'" for district in top_districts])

        query = f'''
            SELECT * 
            FROM postcode_district_boundaries
            WHERE district IN ({formatted_list})
        '''
        gdf = gpd.read_postgis(query, con=con, geom_col='geometry')
        gdf['centroid'] = gdf.geometry.centroid

        combined = gdf.merge(df_year, how='left', left_on='district', right_on='cardholder_location')
        yearly_combined[year] = combined

# Pickle the dictionary
with open(os.path.join(output_dir, 'yearly_combined2.pkl'), 'wb') as f:
    pickle.dump(yearly_combined, f)

# Compute global min and max for 'merchant_index_spend'
all_spend_values = pd.concat([gdf['merchant_index_spend'] for gdf in yearly_combined.values()])
global_vmin = all_spend_values.min()
global_vmax = all_spend_values.max()


# Plotting faceted maps
n_years = len(years)
fig, axes = plt.subplots(n_years, 1, figsize=(10, 6 * n_years))

if n_years == 1:
    axes = [axes]

for ax, year in zip(axes, years):
    gdf = yearly_combined[year]
    gdf.plot(
        ax=ax, 
        column='merchant_index_spend', 
        legend=True, 
        legend_kwds={'label': f'Index of spending in E8 ({year} Q1)'},
        alpha=0.5, 
        edgecolor='black',
        vmin = global_vmin,
        vmax = global_vmax
    )

    # Add district labels
    for _, row in gdf.iterrows():
        x = row['centroid'].x
        y = row['centroid'].y
        ax.text(x, y, row['district'], fontsize=8, ha='center', va='center')

    range_top = gdf['distance_km'].max()
    ax.text(
        ax.get_xlim()[1],
        ax.get_ylim()[0],
        s=f"Top {top_pc}% range = {round(range_top,2)}km",
        ha='right',
        va='top'
    )

    cx.add_basemap(ax, crs=gdf.crs, source=cx.providers.OpenStreetMap.Mapnik)
    ax.set_title(f"Top {top_pc}% Districts - {year} Q1", fontsize=14)
    ax.set_axis_off()

plt.tight_layout()
plt.show()


fig.savefig(os.path.join('..','outputs','session3','choro_spending_e8.png'), dpi=600)
```

```{python}
import matplotlib.pyplot as plt
from geopy.distance import geodesic
import geopandas as gpd
import contextily as cx
import numpy as np
import psycopg2
import os
import pickle

top_pc = 5
years = sorted(df3['year'].unique())
n_years = len(years)

### Step 1: Prepare map data ###
yearly_combined = {}

with psycopg2.connect(**db_params) as con:
    for year in years:
        df_year = df3[(df3['year'] == year) & (df3['quarter'] == 1)].copy()
        df_year = df_year.drop_duplicates(subset=['cardholder_location'])

        top_threshold = np.quantile(df_year['merchant_index_spend'], 1 - (top_pc / 100))
        top_districts = df_year[df_year['merchant_index_spend'] >= top_threshold]['cardholder_location'].tolist()
        formatted_list = ', '.join([f"'{district}'" for district in top_districts])

        query = f'''
            SELECT * 
            FROM postcode_district_boundaries
            WHERE district IN ({formatted_list})
        '''
        gdf = gpd.read_postgis(query, con=con, geom_col='geometry')
        gdf['centroid'] = gdf.geometry.centroid
        combined = gdf.merge(df_year, how='left', left_on='district', right_on='cardholder_location')
        yearly_combined[year] = combined

# Global vmin/vmax for consistent color scale
all_spend_values = pd.concat([gdf['merchant_index_spend'] for gdf in yearly_combined.values()])
global_vmin = all_spend_values.min()
global_vmax = all_spend_values.max()

### Step 2: Set up 5x2 layout ###
fig, axes = plt.subplots(nrows=n_years, ncols=2, figsize=(16, 6 * n_years))
if n_years == 1:
    axes = np.array([axes])  # Ensure 2D even for 1 row

### Step 3: Plot each year ###
for i, year in enumerate(years):
    ax_scatter = axes[i, 0]
    ax_map = axes[i, 1]

    ### LEFT: SCATTER ###
    df_year = df3[(df3['year'] == year) & (df3['quarter'] == 1)].copy()
    avg_val = df_year['merchant_index_spend'].mean()
    
    ax_scatter.scatter(df_year['distance_km'], df_year['merchant_index_spend'], alpha=0.6)
    ax_scatter.axhline(y=avg_val, color='r', linestyle='dashed')

    # Label top 5%
    top_threshold = np.quantile(df_year['merchant_index_spend'], 1 - (top_pc / 100))
    top_df = df_year[df_year['merchant_index_spend'] >= top_threshold]
    range_top = top_df['distance_km'].max()

    for _, row in top_df.iterrows():
        ax_scatter.text(
            row['distance_km'],
            row['merchant_index_spend'],
            row['cardholder_location'],
            fontsize=8,
            alpha=0.7
        )

    ax_scatter.set_title(f"Distance vs Spend (E8) - Q1 {year}")
    ax_scatter.set_xlabel("Distance (km)")
    ax_scatter.set_ylabel("Merchant Index Spend")

    ax_scatter.text(
        ax_scatter.get_xlim()[1],
        ax_scatter.get_ylim()[0]*5,
        s=f"Red dashed line = average ({round(avg_val, 2)})\nTop {top_pc}% labelled",
        size=8,
        ha='right'
    )

    ax_scatter.text(
        ax_scatter.get_xlim()[1] * .95,
        ax_scatter.get_ylim()[1] * .95,
        s=f"Top {top_pc}% range = {round(range_top,2)}km\n# top areas = {len(top_df)}\nTotal areas = {len(df_year)}",
        ha='right',
        va='top'
    )

    ### RIGHT: MAP ###
    gdf = yearly_combined[year]
    gdf.plot(
        ax=ax_map,
        column='merchant_index_spend',
        legend=True,
        legend_kwds={'label': f'Spending Index - Q1 {year}'},
        alpha=0.5,
        edgecolor='black',
        vmin=global_vmin,
        vmax=global_vmax
    )

    for _, row in gdf.iterrows():
        x = row['centroid'].x
        y = row['centroid'].y
        ax_map.text(x, y, row['district'], fontsize=8, ha='center', va='center')

    cx.add_basemap(ax_map, crs=gdf.crs, source=cx.providers.OpenStreetMap.Mapnik)
    ax_map.set_title(f"Top {top_pc}% Cardholder Districts - Q1 {year}")
    ax_map.set_axis_off()

    range_top_map = gdf['distance_km'].max()
    ax_map.text(
        ax_map.get_xlim()[1],
        ax_map.get_ylim()[0],
        s=f"Top {top_pc}% range = {round(range_top_map,2)}km",
        ha='right',
        va='top'
    )

plt.tight_layout()
plt.show()

```
```{python}
# test = e8_merchant.groupby(['year','quarter','merchant_location'])['merchant_index_spend'].sum().reset_index()

# test4 = q1_2019.groupby(['time_period_value','merchant_location'])['merchant_index_spend'].sum().reset_index()
```
---
title: "Untitled"
format: html
---


```{python}
import pandas as pd
import requests
from bs4 import BeautifulSoup
import re
import geopandas as gpd
from shapely.geometry import Point
import matplotlib.pyplot as plt
import contextily as cx
from adjustText import adjust_text
import configparser
import psycopg2
import os
import pickle
import numpy as np
from sklearn.neighbors import BallTree, radius_neighbors_graph
from scipy.spatial import cKDTree
import janitor

config = configparser.ConfigParser()
config.read(os.path.join('..', 'db_config_jk.ini'))

db_params = dict(config['postgresql'])
```

Read the table on wikipedia: [https://en.wikipedia.org/wiki/List_of_Premier_League_stadiums](https://en.wikipedia.org/wiki/List_of_Premier_League_stadiums)

```{python}
# Define the URL
url = "https://en.wikipedia.org/wiki/List_of_Premier_League_stadiums"

# Send GET request to the webpage
response = requests.get(url)
response.raise_for_status()  # Raise an exception for bad status codes

# Parse the HTML content
soup = BeautifulSoup(response.content, 'html.parser')

# Find the first wikitable (which contains the stadiums data)
table = soup.find('table', {'class': 'wikitable'})

# Extract table data using pandas
stadiums_df = pd.read_html(str(table))[0]

# Clean the data
def clean_text(text):
    """Remove Wikipedia reference numbers and extra whitespace"""
    if pd.isna(text):
        return text
    # Remove reference numbers like [1], [2], etc.
    text = re.sub(r'\[.*?\]', '', str(text))
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Apply cleaning to all columns
for col in stadiums_df.columns:
    stadiums_df[col] = stadiums_df[col].apply(clean_text)

# Remove any completely empty rows
stadiums_df = stadiums_df.dropna(how='all')

# Display basic info about the scraped data
print(f"Successfully scraped {len(stadiums_df)} stadiums")
print(f"Columns: {', '.join(stadiums_df.columns)}")
print("\nFirst few rows:")
print(stadiums_df.head())

# Display data types and info
print("\nDataFrame info:")
print(stadiums_df.info())

# Optional: Save to CSV
stadiums_df.to_csv('premier_league_stadiums.csv', index=False)
print("\nData saved to 'premier_league_stadiums.csv'")

# Parse the coordinates into usable WGS64 geometries
def parse_coordinates(coord_str):
    """
    Convert coordinates to decimal degrees (WGS84)
    Handles format: 53°25′51″N 002°57′39″W﻿ / ﻿53.43083°N 2.96083°W
    """
    if pd.isna(coord_str) or coord_str == '':
        return None, None
    
    try:
        # Debug: Show what we're trying to parse
        print(f"DEBUG: Parsing coordinate string: '{coord_str}'")
        print(f"DEBUG: Length: {len(str(coord_str))}")
        print(f"DEBUG: Repr: {repr(str(coord_str))}")
        
        # Clean the string - remove extra whitespace and invisible characters
        coord_str = re.sub(r'\s+', ' ', str(coord_str).strip())
        coord_str = coord_str.replace('\ufeff', '')  # Remove BOM character if present
        coord_str = coord_str.replace('\u200f', '')  # Remove right-to-left mark
        coord_str = coord_str.replace('\u200e', '')  # Remove left-to-right mark
        
        print(f"DEBUG: After cleaning: '{coord_str}'")
        
        # Check if there's a "/" separator (indicating both DMS and decimal formats)
        if '/' in coord_str:
            print("DEBUG: Found '/' separator, trying decimal part")
            # Split and use the decimal part (after the "/")
            decimal_part = coord_str.split('/')[-1].strip()
            print(f"DEBUG: Decimal part: '{decimal_part}'")
            
            # More flexible pattern for decimal coordinates
            # Matches: 53.43083°N 2.96083°W or 53.43083°N, 2.96083°W
            decimal_patterns = [
                r'(\d+\.?\d*)°([NS])\s+(\d+\.?\d*)°([EW])',
                r'(\d+\.?\d*)°([NS])[,\s]+(\d+\.?\d*)°([EW])',
                r'(\d+\.?\d*)\s*°\s*([NS])\s+(\d+\.?\d*)\s*°\s*([EW])'
            ]
            
            for pattern in decimal_patterns:
                decimal_match = re.search(pattern, decimal_part)
                if decimal_match:
                    print(f"DEBUG: Decimal pattern matched: {decimal_match.groups()}")
                    lat_val, lat_dir, lon_val, lon_dir = decimal_match.groups()
                    lat_decimal = float(lat_val)
                    lon_decimal = float(lon_val)
                    
                    # Apply direction (negative for South/West)
                    if lat_dir == 'S':
                        lat_decimal = -lat_decimal
                    if lon_dir == 'W':
                        lon_decimal = -lon_decimal
                        
                    print(f"DEBUG: Converted to: {lat_decimal}, {lon_decimal}")
                    return lat_decimal, lon_decimal
        
        # Fallback: Try to parse DMS format from the beginning
        print("DEBUG: Trying DMS format")
        # More flexible DMS patterns
        dms_patterns = [
            r'(\d+)°(\d+)′(\d+)″([NS])\s+(\d+)°(\d+)′(\d+)″([EW])',
            r'(\d+)°(\d+)′(\d+)″([NS])[,\s]+(\d+)°(\d+)′(\d+)″([EW])',
            r'(\d+)\s*°\s*(\d+)\s*′\s*(\d+)\s*″\s*([NS])\s+(\d+)\s*°\s*(\d+)\s*′\s*(\d+)\s*″\s*([EW])'
        ]
        
        for pattern in dms_patterns:
            dms_match = re.search(pattern, coord_str)
            if dms_match:
                print(f"DEBUG: DMS pattern matched: {dms_match.groups()}")
                lat_deg, lat_min, lat_sec, lat_dir, lon_deg, lon_min, lon_sec, lon_dir = dms_match.groups()
                
                # Convert to decimal degrees
                lat_decimal = int(lat_deg) + int(lat_min)/60 + int(lat_sec)/3600
                lon_decimal = int(lon_deg) + int(lon_min)/60 + int(lon_sec)/3600
                
                # Apply direction (negative for South/West)
                if lat_dir == 'S':
                    lat_decimal = -lat_decimal
                if lon_dir == 'W':
                    lon_decimal = -lon_decimal
                    
                print(f"DEBUG: Converted to: {lat_decimal}, {lon_decimal}")
                return lat_decimal, lon_decimal
        
        # Last attempt: simple decimal format
        print("DEBUG: Trying simple decimal format")
        simple_patterns = [
            r'([-+]?\d*\.?\d+)[,\s]+([-+]?\d*\.?\d+)',
            r'(\d+\.?\d+)\s*,\s*(\d+\.?\d+)',
            r'(\d+\.?\d+)\s+(\d+\.?\d+)'
        ]
        
        for pattern in simple_patterns:
            simple_match = re.search(pattern, coord_str)
            if simple_match:
                print(f"DEBUG: Simple decimal pattern matched: {simple_match.groups()}")
                lat, lon = simple_match.groups()
                result = float(lat), float(lon)
                print(f"DEBUG: Converted to: {result}")
                return result
        
        print("DEBUG: No patterns matched")
            
    except Exception as e:
        print(f"ERROR parsing coordinates '{coord_str}': {e}")
        
    return None, None

# Debug: Print all column names to see what's available
print("All columns in the dataframe:")
for i, col in enumerate(stadiums_df.columns):
    print(f"{i}: '{col}'")

# Find coordinates column and convert to WGS84
coord_columns = [col for col in stadiums_df.columns if col == 'Coordinates'] 

coord_col = coord_columns[0]  # Use first coordinates column found
print(f"Using coordinates column: '{coord_col}'")

# Debug: Show some raw coordinate values
print(f"\nSample raw coordinate values from '{coord_col}':")
sample_coords = stadiums_df[coord_col].dropna().head(5)
for i, coord in enumerate(sample_coords):
    print(f"{i+1}: '{coord}'")

# Extract and convert coordinates
coords = stadiums_df[coord_col].apply(parse_coordinates)

# Debug: Show parsing results
print(f"\nParsing results (first 5):")
for i, coord in enumerate(coords.head(5)):
    original = stadiums_df[coord_col].iloc[i] if i < len(stadiums_df) else "N/A"
    print(f"{i+1}: '{original}' -> {coord}")

# Create separate latitude and longitude columns
stadiums_df['Latitude_WGS84'] = [coord[0] if coord[0] is not None else None for coord in coords]
stadiums_df['Longitude_WGS84'] = [coord[1] if coord[1] is not None else None for coord in coords]

# Display conversion results
successful_conversions = stadiums_df[['Latitude_WGS84', 'Longitude_WGS84']].dropna()
print(f"\nSuccessfully converted {len(successful_conversions)} coordinates to WGS84 decimal degrees")

if len(successful_conversions) > 0:
    print("\nSample WGS84 coordinates:")
    print(successful_conversions.head())
else:
    print("No coordinates were successfully converted. Check the parsing function.")

# Optional: Display some basic statistics
print(f"\nSample of stadium names:")
if 'Stadium' in stadiums_df.columns:
    print(stadiums_df['Stadium'].head(5).tolist())
elif len(stadiums_df.columns) > 0:
    print(stadiums_df.iloc[:5, 0].tolist())  # First column if 'Stadium' not found
```


```{python}
# Create GeoPandas GeoDataFrame
successful_coords = stadiums_df.dropna(subset=['Latitude_WGS84', 'Longitude_WGS84'])

if len(successful_coords) > 0:
    print(f"\nCreating GeoPandas GeoDataFrame with {len(successful_coords)} stadiums...")
    
    # Create Point geometries from lat/lon coordinates
    geometry = [Point(lon, lat) for lon, lat in zip(successful_coords['Longitude_WGS84'], 
                                                   successful_coords['Latitude_WGS84'])]
    
    # Create GeoDataFrame
    gdf = gpd.GeoDataFrame(successful_coords, geometry=geometry, crs='EPSG:4326')
    
    print(f"GeoPandas GeoDataFrame created successfully!")
    print(f"CRS: {gdf.crs}")
    print(f"Geometry type: {gdf.geometry.geom_type.iloc[0] if len(gdf) > 0 else 'None'}")
    
    # Display basic info about the GeoDataFrame
    print(f"\nGeoDataFrame info:")
    print(f"Shape: {gdf.shape}")
    print(f"Columns: {list(gdf.columns)}")
    
    # Show first few rows with geometry
    print(f"\nFirst few stadiums with coordinates:")
    if 'Stadium' in gdf.columns:
        display_cols = ['Stadium', 'Latitude_WGS84', 'Longitude_WGS84', 'geometry']
        available_cols = [col for col in display_cols if col in gdf.columns]
        print(gdf[available_cols].head())
    else:
        print(gdf[['Latitude_WGS84', 'Longitude_WGS84', 'geometry']].head())
    
    # Optional: Save as GeoJSON
    gdf.to_file('premier_league_stadiums.geojson', driver='GeoJSON')
    print(f"\nGeoDataFrame saved as 'premier_league_stadiums.geojson'")
    
    # Optional: Save as Shapefile
    try:
        gdf.to_file('premier_league_stadiums.shp')
        print(f"GeoDataFrame saved as 'premier_league_stadiums.shp'")
    except Exception as e:
        print(f"Could not save as Shapefile: {e}")
    
    # Calculate bounding box
    bounds = gdf.total_bounds
    print(f"\nBounding box (min_lon, min_lat, max_lon, max_lat): {bounds}")
    
    # Optional: Basic spatial analysis
    print(f"\nBasic spatial info:")
    print(f"Centroid of all stadiums: {gdf.geometry.centroid.iloc[0] if len(gdf) > 0 else 'None'}")
    
    # Return the GeoDataFrame for further use
    stadiums_gdf = gdf
    
else:
    print("No valid coordinates found - cannot create GeoDataFrame")
    stadiums_gdf = None
```


```{python}
# Filter to just stadiums that are currently open
stadiums_gdf = stadiums_gdf.loc[stadiums_gdf['Closed'].isna()]
```

# Plot

```{python}
# Create figure and axis
fig, ax = plt.subplots(figsize=(10, 8))  # Note: subplots, not plt, and figsize should be reasonable

# Plot the stadiums
stadiums_gdf.plot(ax=ax, marker='o', color='red', markersize=50, alpha=0.7)

# Add some styling
ax.set_title('Premier League Stadiums', fontsize=16, fontweight='bold')
ax.set_axis_off()
cx.add_basemap(ax, crs = stadiums_gdf.crs, source=cx.providers.OpenStreetMap.Mapnik)

# Create text objects for labels
# texts = []
# for idx, row in stadiums_gdf.iterrows():
#     text = ax.text(row.geometry.x, row.geometry.y, row['Stadium'],
#                    fontsize=8, ha='center', va='center',
#                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))
#     texts.append(text)

# # Adjust text positions to avoid overlaps
# adjust_text(texts, 
#             arrowprops=dict(arrowstyle='->', color='black', lw=0.5),
#             expand_points=(1.2, 1.2),
#             expand_text=(1.2, 1.2))

# Show the plot
plt.tight_layout()
plt.show()
```

```{python}
tott = stadiums_gdf.loc[stadiums_gdf['Stadium'] == 'Tottenham Hotspur Stadium']

fig, ax = plt.subplots(figsize=(10, 8)) 

# Plot the stadiums
tott.plot(ax=ax, marker='o', color='red', markersize=50, alpha=0.7)


# Add some styling
ax.set_title('Premier League Stadiums', fontsize=16, fontweight='bold')


# Get the coordinates
x, y = tott.geometry.x.iloc[0], tott.geometry.y.iloc[0]
# Set reasonable axis limits around the point
margin = 0.01  # Adjust this value to zoom in/out
ax.set_xlim(x - margin, x + margin)
ax.set_ylim(y - margin, y + margin)

ax.set_axis_off()

cx.add_basemap(ax, crs = tott.crs, source=cx.providers.OpenStreetMap.Mapnik)
plt.show()
```


```{python}

file = 'msoa_geometries_restored.pkl'
path = os.path.join('..', 'data', file)
if(os.path.isfile(path)) == False:
    print('Downloading geometries')
    # Get msoa geometries for the whole country, join with lad21 and rgn21
    query = '''SELECT foo.msoa21cd, foo.msoa21nm, poo.lad21nm, poo.rgn21nm, foo.geometry 
            FROM msoa21_boundaries foo
            LEFT JOIN (
            SELECT DISTINCT msoa21cd, lad21cd 
            FROM pcode_census21_lookup) loo
            ON foo.msoa21cd = loo.msoa21cd
            LEFT JOIN 
            (SELECT DISTINCT lad21nm, lad21cd, rgn21nm FROM lad21_lookup) poo
            ON loo.lad21cd = poo.lad21cd
    '''

    with psycopg2.connect(**db_params) as con:
        msoa_gdf = gpd.read_postgis(query, con = con, geom_col='geometry')

    msoa_gdf.to_pickle(path)
else:
    print('Geometries already in directory. Loading.')
    with open(path, 'rb') as file:
        msoa_gdf = pickle.load(file)
```

Use spatial join to locate stadiums in MSOAs

```{python}
# subset to only the current premier league satdiums
clubs = [
    'Emirates Stadium',
    'Villa Park',
    'Dean Court',
    'Brentford Community Stadium',
    'Falmer Stadium',
    'Stamford Bridge',
    'Selhurst Park',
    'Goodison Park',
    'Craven Cottage',
    'Portman Road',
    'King Power Stadium Formerly Walkers Stadium',
    'Anfield',
    'City of Manchester Stadium',
    'Old Trafford',
    "St James' Park",
    'City Ground',
    "St Mary's Stadium",
    'Tottenham Hotspur Stadium',
    'London Stadium Formerly Olympic Stadium',
    'Molineux Stadium'
]

points_gdf = gpd.GeoDataFrame(stadiums_gdf.loc[stadiums_gdf['Stadium'].isin(clubs), ['Stadium', 'geometry']]).reset_index(drop=True)

# points_gdf = gpd.GeoDataFrame(stadiums_gdf[['geometry']])
points_gdf = points_gdf.to_crs(epsg=27700) # maybe go back to web scraping and redefine the desired crs to 27700

# Spatial join to find which polygon each point falls in
joined = gpd.sjoin(points_gdf, msoa_gdf, how='left', predicate='within')
```

# Finding nearest neighbours

```{python}
# # Get point coordinates (n_targets x 2)
# target_coords = np.array([[geom.x, geom.y] for geom in joined.geometry])

# # Get MSOA centroid coordinates (n_msoas x 2)
# msoa_gdf['centroid'] = msoa_gdf.geometry.centroid
# msoa_coords = np.array([[pt.x, pt.y] for pt in msoa_gdf['centroid']])

# # Radius in meters
# radius_m = 1000

# # Build KD-tree for fast spatial query
# tree = cKDTree(msoa_coords)

# # Query neighbors within radius for each point
# neighbors_dict = {}

# for i, point in enumerate(target_coords):
#     indices = tree.query_ball_point(point, r=radius_m)
#     neighbor_codes = msoa_gdf.iloc[indices]['msoa21cd'].tolist()
#     point_id = joined.iloc[i].get('Stadium', f'point_{i}')  # Use a unique ID or fallback
#     neighbors_dict[point_id] = neighbor_codes


# results = []
# for i, row in joined.iterrows():
#     point_id = row.get('Stadium', f'point_{i}')
#     containing_msoa = row['msoa21cd']  # MSOA containing the point (may be NaN if outside)
#     neighbors = neighbors_dict.get(point_id, [])
#     results.append({
#         'stadium': point_id,
#         'containing_msoa': containing_msoa,
#         'neighboring_msoas': neighbors
#     })

```

```{python}
# Get point coordinates (n_targets x 2)
target_coords = np.array([[geom.x, geom.y] for geom in joined.geometry])

# Radius in meters
radius_m = 1000

# Method 1: Using buffer and intersects (Recommended - most accurate)
neighbors_dict = {}
for i, row in joined.iterrows():
    point_geom = row.geometry
    point_id = row.get('Stadium', f'point_{i}')
    
    # Create buffer around the point
    point_buffer = point_geom.buffer(radius_m)
    
    # Find all MSOAs that intersect with the buffer
    intersecting_msoas = msoa_gdf[msoa_gdf.geometry.intersects(point_buffer)]
    neighbor_codes = intersecting_msoas['msoa21cd'].tolist()
    
    neighbors_dict[point_id] = neighbor_codes

# Alternative Method 2: Using distance calculation (also accurate but potentially slower)
# Uncomment below if you prefer this approach:

# neighbors_dict = {}
# for i, row in joined.iterrows():
#     point_geom = row.geometry
#     point_id = row.get('Stadium', f'point_{i}')
#     
#     # Calculate distance from point to each MSOA geometry
#     distances = msoa_gdf.geometry.distance(point_geom)
#     
#     # Find MSOAs within the radius
#     within_radius = distances <= radius_m
#     neighbor_codes = msoa_gdf[within_radius]['msoa21cd'].tolist()
#     
#     neighbors_dict[point_id] = neighbor_codes

# Rest of your code remains the same
results = []
for i, row in joined.iterrows():
    point_id = row.get('Stadium', f'point_{i}')
    containing_msoa = row['msoa21cd']  # MSOA containing the point (may be NaN if outside)
    neighbors = neighbors_dict.get(point_id, [])
    results.append({
        'stadium': point_id,
        'containing_msoa': containing_msoa,
        'neighboring_msoas': neighbors
    })
```

```{python}
# # Collect all MSOA codes from containing_msoa and neighboring_msoas
# all_msoas = []

# for res in results:
#     # Add containing MSOA if it exists and is not None or NaN
#     if res['containing_msoa']:
#         all_msoas.append(res['containing_msoa'])
#     # Add all neighboring MSOAs
#     all_msoas.extend(res['neighboring_msoas'])

# # Get unique MSOAs by converting to a set, then back to a list if needed
# unique_msoas = list(set(all_msoas))

# # unique_msoas now holds all unique MSOA codes from both categories
# print(unique_msoas)

# neighbors_gdf = msoa_gdf.loc[msoa_gdf['msoa21cd'].isin(unique_msoas)]
```

```{python}
from collections import defaultdict
# Step 1: Build a mapping from MSOA to associated stadium(s)
msoa_to_stadiums = defaultdict(set)
for res in results:
    stadium = res['stadium']
    if res['containing_msoa']:
        msoa_to_stadiums[res['containing_msoa']].add(stadium)
    for neighbor in res['neighboring_msoas']:
        # print(neighbor)
        msoa_to_stadiums[neighbor].add(stadium)

# Step 2: Flatten the mapping into a DataFrame
msoa_stadium_rows = []
for msoa, stadiums in msoa_to_stadiums.items():
    for stadium in stadiums:
        msoa_stadium_rows.append({
            'msoa21cd': msoa,
            'stadium': stadium
        })

msoa_stadium_df = pd.DataFrame(msoa_stadium_rows)

# Step 3: Join this info with the original MSOA GeoDataFrame
neighbors_gdf = msoa_gdf.merge(msoa_stadium_df, on='msoa21cd', how='inner')
```

```{python}
# Plot Haringey to check
haringey = neighbors_gdf.loc[neighbors_gdf['stadium'].str.contains('Tottenham')]
tott = tott.to_crs(epsg=27700)

fig, ax = plt.subplots(figsize=[8,8])

haringey.plot(ax=ax, alpha=0.5, edgecolor="black")
tott.plot(ax=ax, marker='o', color='red', markersize=50, alpha=0.7)
cx.add_basemap(ax, crs = tott.crs, source=cx.providers.OpenStreetMap.Mapnik)

```

# Child poverty data

```{python}
relative = pd.read_csv(os.path.join('..','data','relative_msoa_u15-23-24_codes.csv'), skiprows=9, skipfooter=15)

relative = relative.iloc[:,[1,2,4]]
relative.columns = ['msoa_code','age','count']
relative = relative.loc[relative['age']=='Total']
relative = relative.drop(columns='age')
relative['count'] = relative['count'].replace('..', 0).astype(int)


absolute = pd.read_csv(os.path.join('..','data','absolute_low_income_msoa.csv'), skiprows=9)

absolute = absolute.iloc[:,[1,3]]
absolute.columns = ['msoa_name','count']
```

```{python}
# Get population estimates by age (thank you Ollie for having this in db!)

# with psycopg2.connect(**db_params) as con:
#     query = '''
#             SELECT msoa21nm, msoa21cd, SUM(count) AS population
#             FROM census21_msoa_population_by_age 
#             WHERE age < 16
#             GROUP BY msoa21nm, msoa21cd
#     '''
#     pop_ests = pd.read_sql(query, con=con)


# alternative avoiding db
# url = 'https://www.nomisweb.co.uk/api/v01/dataset/NM_2027_1.data.csv?date=latest&geography=637536588...637536596,637540863,637541007,637536597...637536614,637540768,637536615...637536631,637540769,637540864,637536632...637536653,637541008...637541011,637536654...637536666,637541006,637538323...637538386,637540972,637539688...637539722,637541053...637541055,637535843...637535865,637540848,637541043...637541052,637535866...637535954,637535819...637535842,637540798,637540799,637541042,637536667...637536744,637537855...637537883,637537896...637537916,637540980,637540981,637537838...637537854,637537884...637537895,637537917...637537934,637538008...637538071,637539184...637539194,637540836,637539195...637539243,637540826,637539244...637539254,637540953,637539255...637539286,637540839,637539287...637539332,637535141...637535249,637540856,637540865...637540870,637540933...637540936,637535250...637535281,637540950,637540951,637535282...637535333,637540908...637540914,637535334...637535551,637540883...637540885,637535552...637535573,637540886,637540887,637535574...637535653,637536745...637536774,637540770,637541027,637541028,637536775...637536814,637540771,637540846,637540847,637536815...637536835,637541029,637536836...637536882,637539723...637539728,637540989,637539729...637539738,637540990,637540991,637539739...637539770,637540992,637540993,637539771...637539794,637535654...637535753,637540952,637535754...637535818,637540760,637540800,637540801,637540824,637540825,637536295...637536353,637540896...637540899,637536354...637536542,637540809,637540817,637540830,637540831,637536543...637536587,637536883...637536944,637540772,637540774,637540776,637540807,637540808,637540915,637540916,637536945...637536982,637540791,637540858,637540859,637541030,637539601...637539606,637539617...637539636,637539678...637539687,637540818,637540819,637541018,637541019,637539607...637539616,637539637...637539677,637541020,637541021,637538072...637538086,637540784,637538087...637538117,637540979,637538118...637538131,637540785,637538132...637538141,637540827,637538142...637538153,637540761,637538154...637538163,637540872,637540873,637539333...637539343,637540777,637539344...637539374,637540773,637539375...637539411,637540775,637540917,637540918,637539412...637539417,637540820,637540821,637539418...637539457,637540822,637540823,637539458...637539495,637539795...637539823,637540857,637539824...637539836,637540860,637539837...637539850,637540792,637539851...637539891,637536983...637537027,637540998,637540999,637537028...637537059,637540925,637539976...637540014,637540086...637540153,637540932,637540154...637540192,637540430...637540452,637540763,637540453...637540463,637540996,637540997,637540464...637540493,637540654...637540738,637535955...637536075,637540764,637540766,637540850...637540855,637540986...637540988,637536076...637536116,637540762,637536117...637536195,637540767,637540894,637540895,637536196...637536223,637540765,637536224...637536294,637540849,637537300...637537319,637540832,637540833,637537320...637537339,637540888,637540889,637537340...637537372,637540816,637540878,637540954,637540955,637537668...637537686,637540960,637540961,637537651...637537667,637537687...637537702,637537768...637537779,637541034,637541035,637537780...637537788,637540782,637537789...637537837,637540828,637540829,637541033,637538454...637538534,637540928,637540929,637538535...637538552,637540875,637541004,637541005,637538553...637538597,637540930,637540931,637538598...637538624,637538861...637538895,637538913...637538939,637541036,637541037,637538970...637538993,637539496...637539575,637540982,637540983,637539576...637539586,637540861,637540862,637541000,637541001,637539587...637539600,637541002,637541003,637540193...637540203,637540210...637540237,637538940...637538958,637541038,637538994...637539008,637541040,637538896...637538912,637541041,637538959...637538969,637541039,637540252...637540281,637540204...637540209,637540238...637540251,637540783,637534209...637534230,637540756,637534231...637534270,637540904,637540905,637534271...637534297,637540743,637534298...637534330,637540906,637540907,637534331...637534366,637540740,637540745,637540747,637534367...637534392,637541063,637534393...637534434,637540746,637540937,637540938,637534435...637534470,637540749,637540919...637540922,637534471...637534505,637540751,637534506...637534531,637540744,637540879...637540882,637540941...637540944,637534532...637534555,637540871,637540874,637541056...637541059,637534556...637534615,637540752,637534616...637534643,637540753,637540837,637534644...637534704,637540754,637534705...637534730,637540750,637540923,637540924,637534731...637534828,637540758,637534829...637534860,637540741,637540742,637540755,637540958,637540959,637534861...637534920,637540945...637540949,637534921...637534946,637540757,637540876,637540877,637540967...637540970,637534947...637535000,637540759,637540892,637540893,637535001...637535023,637540793,637535024...637535052,637540810,637540811,637541060...637541062,637535053...637535116,637541031,637541032,637535117...637535140,637537373...637537464,637540890,637540891,637537465...637537477,637541016,637541017,637537478...637537545,637540962...637540966,637537546...637537601,637540778,637540926,637540927,637537602...637537632,637541022,637541023,637537633...637537650,637537703...637537767,637540780,637540781,637538387...637538397,637540814,637540815,637538398...637538453,637538699...637538733,637540786,637540795,637538734...637538797,637540779,637540787,637540788,637538798...637538846,637541024...637541026,637538847...637538860,637539009...637539021,637540994,637540995,637539022...637539038,637540812,637540813,637539039...637539111,637540789,637539112...637539123,637540834,637540835,637539124...637539169,637540790,637539170...637539183,637539892...637539927,637540971,637541064,637539928...637539960,637540841,637539961...637539975,637540282...637540307,637540794,637540308...637540401,637540748,637540402...637540417,637540796,637540418...637540429,637540494...637540593,637537060...637537135,637540842...637540845,637540900...637540903,637537136...637537159,637540802,637540803,637537160...637537189,637541012...637541015,637537190...637537220,637540939,637540940,637537221...637537236,637540797,637537276...637537299,637540804...637540806,637537935...637538007,637540739,637540594...637540653,637540973...637540976,637537237...637537275,637538270...637538276,637540838,637540840,637538277...637538322,637540956,637540957,637538164...637538182,637540977,637540978,637538183...637538269,637538625...637538698,637540984,637540985,637540015...637540085,637541065...637541119,637541464,637541120...637541154,637541468...637541470,637541172...637541179,637541465,637541180...637541219,637541463,637541220...637541248,637541472,637541249...637541264,637541462,637541471,637541265...637541298,637541413...637541458,637541466,637541467,637541299...637541329,637541336...637541412,637541155...637541171,637541459,637541461,637541330...637541335,637541460&c2021_age_102=0,1001...1003&measures=20100'


# # Check if it's already downloaded
# filepath = os.path.join('..', 'data', 'msoa_u15_pop.csv')
# if os.path.isfile(filepath)==False:
#     req = requests.get(url)
#     with open(filepath, 'wb') as output_file:
#         output_file.write(req.content)
# else:
#     print('Data already acquired. Loading it')

# pop_ests = pd.read_csv(filepath)

# pop_ests = pop_ests.rename(columns={
#     'geography':'msoa21nm',
#     'geography_code': 'msoa21cd',
#     'obs_value': 'population'})

# # Just get cols of interest
# pop_ests = pop_ests[['msoa21nm','msoa21cd','c2021_age_102','population']]

# # Drop the 'all usual residents' row for each msoa
# pop_ests = pop_ests.loc[pop_ests['c2021_age_102']>0]

# # get the sum of the age groups
# pop_ests = pop_ests.groupby(['msoa21nm', 'msoa21cd'])['population'].sum().reset_index()

# Fourht try - above nomis reuqest misses data for some reason
filepath = os.path.join('..', 'data', 'census2021-ts007-msoa.csv')
pop_ests = pd.read_csv(filepath)

pop_ests = janitor.clean_names(pop_ests)
pop_ests = pop_ests.rename(columns={
    'geography':'msoa21nm',
    'geography_code': 'msoa21cd'})

pop_ests.columns = (
    pop_ests.columns
    .str.replace(';_measures_value', '', regex=True)   # Remove ','

)

# Just get cols of interest
pop_ests = pop_ests[['msoa21nm','msoa21cd','age_aged_4_years_and_under','age_aged_5_to_9_years','age_aged_10_to_15_years']]

pop_ests['population'] = pop_ests[['age_aged_4_years_and_under','age_aged_5_to_9_years','age_aged_10_to_15_years']].sum(axis=1)

pop_ests = pop_ests[['msoa21nm','msoa21cd','population']]
```

```{python}

# Get MSOA11 to MSOA21 lookup
import time

def fetch_data(base_url, where_clause, result_offset, max_records):
    params = {
        'where': where_clause,
        'outFields': '*',
        'outSR': '4326',
        'f': 'json',
        'resultOffset': result_offset,
        'resultRecordCount': max_records
    }

    response = requests.get(base_url, params=params)
    error_504 = False

    if len(response.content) < 1000:
        peek_content = response.text
        if "error" in peek_content.lower() and "504" in peek_content:
            error_504 = True

    if response.status_code == 200 and not error_504:
        return response
    else:
        raise Exception(f"Error: Status code {response.status_code} (body may contain 504)")

# Main loop

api_endpoint = 'https://services1.arcgis.com/ESMARspQHYMw9BZ9/ArcGIS/rest/services/MSOA11_MSOA21_LAD22_EW_LU_v2/FeatureServer/0/query'

rep = 1
where_clause = '1=1'
result_offset = 0
max_records = 2000
max_tries = 5
all_results = []

while True:
    print(f"\nStarting page {rep}")
    attempt = 1
    current_max_records = max_records

    while attempt <= max_tries:
        print(f"\nAttempt {attempt}")
        print(f"Trying with max records = {current_max_records}")

        try:
            response = fetch_data(
                base_url=api_endpoint,
                where_clause=where_clause,
                result_offset=result_offset,
                max_records=current_max_records
            )
            print(f"\nSuccess with max records = {current_max_records}")
            break

        except Exception as e:
            print(f"Error: {e}")
            attempt += 1
            current_max_records = int(current_max_records / 2)
            print("Retrying...")
            time.sleep(1)

    if attempt > max_tries:
        print("Max attempts reached. Exiting...")
        break

    json_data = response.json()
    features = json_data.get('features', [])

    if not features:
        break

    # Extract attributes only
    records = [f['attributes'] for f in features]
    df_chunk = pd.DataFrame(records)

    if df_chunk.empty:
        break

    all_results.append(df_chunk)
    result_offset += len(df_chunk)
    rep += 1

# Concatenate all chunks into a single pandas DataFrame
if all_results:
    lookup = pd.concat(all_results, ignore_index=True)
    print(lookup.head())

lookup = janitor.clean_names(lookup)

```

```{python}
# crosswalk data to msoa21
# I'm not 100% sure this is the best way to do this
# It only takes account where msoa11s have merged to msoa21s.
# Doesn't take account where msoa11s have split into msoa21s?
# There are still 122 msoas in CILIF that don't carry over in the merge
# Must be due to splits?
relative2 = relative.merge(lookup, how='left', left_on='msoa_code', right_on='msoa11cd')

relative2 = relative2.groupby(['msoa21cd'])['count'].sum()
# result = relative2.groupby('msoa21cd', as_index=False).agg({'count': 'sum'})

```


```{python}
# Join pop ests to CILIF data
# Relative
relative2 = (relative
            .merge(pop_ests, how = 'left', left_on='msoa_code', right_on='msoa21cd')
            # .drop(columns='msoa_code')
)

# Move count to a better location
cols = list(relative2.columns)
cols.remove('count')
insert_at = cols.index('population')
cols.insert(insert_at, 'count')

relative2 = relative2[cols]

relative2['percentage'] = relative2['count'].div(relative2['population'])*100
relative2['rank'] = relative2['percentage'].rank(method='min', ascending=False)
relative2['decile'] = pd.qcut(relative2['rank'], q=10, labels=list(range(1,11,1)), duplicates='drop')
relative2['decile'] = pd.to_numeric(relative2['decile'], errors='coerce')

# Absolute
absolute2 = (absolute
            .merge(pop_ests, how = 'left', left_on='msoa_name', right_on='msoa21nm')
            .drop(columns='msoa_name')
)

# Move count to a better location
cols = list(absolute2.columns)
cols.remove('count')
insert_at = cols.index('population')
cols.insert(insert_at, 'count')

absolute2 = absolute2[cols]

absolute2['percentage'] = absolute2['count'].div(absolute2['population'])*100
absolute2['rank'] = absolute2['percentage'].rank(method='min')
absolute2['decile'] = pd.qcut(absolute2['percentage'], q=10, labels=list(range(10,0,-1)), duplicates='drop')
absolute2['decile'] = pd.to_numeric(absolute2['decile'], errors='coerce')

```



# Join neighbours with CILIF

```{python}
neighbors_gdf2 = (neighbors_gdf
                  .merge(relative2.loc[:,['msoa21nm','percentage','rank','decile']], how='left', on='msoa21nm')
                  .rename(columns={
                    'percentage':'relative_percentage',
                    'rank': 'relative_rank',
                    'decile': 'relative_decile'
                    })
                  .merge(absolute2.loc[:,['msoa21nm','percentage','rank','decile']], how='left', on='msoa21nm')
                  .rename(columns={
                    'percentage':'absolute_percentage',
                    'rank': 'absolute_rank',
                    'decile': 'absolute_decile'})
                  .sort_values(by='msoa21nm')
)
```


```{python}
# stadiums_gdf2 = stadiums_gdf.to_crs(epsg=27700)
# temp = neighbors_gdf.loc[neighbors_gdf['stadium'].isin(['Anfield', 'Tottenham Hotspur Stadium'])]
# # for stadium in temp['stadium'].unique():
# for stadium in neighbors_gdf2['stadium'].unique():
#     # print(stadium)

#     msoa_temp_gdf = neighbors_gdf2.loc[neighbors_gdf2['stadium']==stadium]
#     stadiums_temp_gdf = stadiums_gdf2.loc[stadiums_gdf2['Stadium']==stadium]

#     fig, ax = plt.subplots(figsize=[8,8])
#     msoa_temp_gdf.plot(ax=ax,column='relative_decile', edgecolor="black", legend = True,
#     legend_kwds={'label': 'Decile'}, 
#     vmin=1, vmax=10,
#     alpha = 0.5)
#     stadiums_temp_gdf.plot(ax=ax, color="red")
#     plt.title(stadium)
#     cx.add_basemap(ax, crs = msoa_temp_gdf.crs, source=cx.providers.OpenStreetMap.Mapnik)
#     ax.set_axis_off()


```

why does tott have 8 points here? in data there are only six
    - resolved. there were always 8 it's just preivously i was only plotting the haringey ones


```{python}
# get lookup again
with psycopg2.connect(**db_params) as con:
    query = ''' SELECT DISTINCT msoa21cd, ladcd as lad21cd, ladnm 
            FROM pcode_census21_lookup
            '''

    msoa_lad_lookup = pd.read_sql(query, con=con)
```

```{python}
relative3 = relative2.merge(msoa_lad_lookup, how='left', on='msoa21cd')
```

# All stadia

```{python}
# use relative2 because this is all MSOAs
decile_boundaries = relative2['rank'].quantile([i/10 for i in range(0, 11)]).tolist()

tott_dots = neighbors_gdf2.loc[neighbors_gdf2['stadium']=='Tottenham Hotspur Stadium']
np_dots = neighbors_gdf2.loc[neighbors_gdf2['msoa21cd']=='E02000398']
non_tott_dots = neighbors_gdf2.loc[neighbors_gdf2['stadium'] !='Tottenham Hotspur Stadium']

las_stads = joined[['lad21nm','Stadium']].drop_duplicates()

la_avg_ranks = relative3.loc[relative3['ladnm'].isin(las_stads['lad21nm'].unique())]
la_avg_ranks = la_avg_ranks.groupby('ladnm')['rank'].mean().reset_index()
# Optional: rename for clarity
la_avg_ranks.rename(columns={'rank': 'average_rank'}, inplace=True)
la_avg_ranks = la_avg_ranks.merge(las_stads,how='left',left_on='ladnm', right_on='lad21nm')

fig, ax = plt.subplots(figsize=[9,9])

# Plot grey points first (bottom layer)

ax.scatter(tott_dots['relative_rank'], tott_dots['stadium'], c='black')
ax.scatter(non_tott_dots['relative_rank'], non_tott_dots['stadium'], c='lightgrey')
ax.scatter(np_dots['relative_rank'], np_dots['stadium'], c='red')
ax.scatter(la_avg_ranks['average_rank'], la_avg_ranks['Stadium'], marker="D",c='blue')

# Add vertical lines for decile boundaries
for i, boundary in enumerate(decile_boundaries):
    ax.axvline(
        x=boundary, 
        color='red', 
        # linestyle=(0, (5, 10)), 
        linestyle='dotted', 
        alpha=0.7)
    if i < 10:
        x_loc = (decile_boundaries[i] + decile_boundaries[i+1]) / 2
        ax.text(x=x_loc, y=ax.get_ylim()[0]*1.2, s=i+1, ha='center')

ax.text(x=ax.get_xlim()[0], y=ax.get_ylim()[0]*1.2, s='Decile', ha='right')
ax.text(0, ax.get_ylim()[1]*1.1, s='Red dot indicates Northumberland Park\nBlack dots indicate MSOAs within 1km of Tottenham Hotspur Stadium\nBlue diamonds indicate the average rank of the borough the Stadium is in', size=8)

# Get the mapping from stadium names to y-axis positions
nudge_x = 0
nudge_y = -0.3
y_labels = ax.get_yticklabels()
y_positions = ax.get_yticks()
label_mapping = {label.get_text(): pos for label, pos in zip(y_labels, y_positions)}

for i, row in la_avg_ranks.iterrows():
    if row['Stadium'] in label_mapping:
        y_pos = label_mapping[row['Stadium']]
        ax.text(x=row['average_rank'] + nudge_x, 
                y=y_pos + nudge_y, 
                s=row['ladnm'], ha='left')

# plt.figtext(0.38, 0.0015, 'Black dots indicate Northumberland Park LSOAs', wrap=False, horizontalalignment='right', fontsize=10)
ax.invert_yaxis()

plt.xlabel('Rank')
# Add title with specific positioning
plt.suptitle(
    'MSOAs within 1km of Premier League Stadiums', fontsize=14, 
    x = 0.7, y=.99, 
    ha='center')  # y controls vertical position
plt.tight_layout()
fig.savefig(os.path.join('..','outputs','all_stadiums_1km.png'), dpi=800, bbox_inches='tight')
plt.show()


```

# Just London stadia 

```{python}
# use relative2 because this is all MSOAs
decile_boundaries = relative2['rank'].quantile([i/10 for i in range(0, 11)]).tolist()

neighbors_gdf3 = neighbors_gdf2.loc[neighbors_gdf2['rgn21nm']=='London']

tott_dots = neighbors_gdf3.loc[neighbors_gdf3['stadium']=='Tottenham Hotspur Stadium']
np_dots = neighbors_gdf3.loc[neighbors_gdf3['msoa21cd']=='E02000398']
non_tott_dots = neighbors_gdf3.loc[neighbors_gdf3['stadium'] !='Tottenham Hotspur Stadium']


las_stads = joined.loc[joined['rgn21nm']=='London', ['lad21nm','Stadium']].drop_duplicates()

la_avg_ranks = relative3.loc[relative3['ladnm'].isin(las_stads['lad21nm'].unique())]
la_avg_ranks = la_avg_ranks.groupby('ladnm')['rank'].mean().reset_index()
# Optional: rename for clarity
la_avg_ranks.rename(columns={'rank': 'average_rank'}, inplace=True)
la_avg_ranks = la_avg_ranks.merge(las_stads,how='left',left_on='ladnm', right_on='lad21nm')

nudge_x = 0
nudge_y = -0.1
fig, ax = plt.subplots(figsize=[9,9])

# Plot grey points first (bottom layer)

ax.scatter(tott_dots['relative_rank'], tott_dots['stadium'], c='black')
ax.scatter(non_tott_dots['relative_rank'], non_tott_dots['stadium'], c='lightgrey')
ax.scatter(np_dots['relative_rank'], np_dots['stadium'], c='red')
ax.scatter(la_avg_ranks['average_rank'], la_avg_ranks['Stadium'], marker="D",c='blue')

# Add vertical lines for decile boundaries
for i, boundary in enumerate(decile_boundaries):
    ax.axvline(
        x=boundary, 
        color='red', 
        # linestyle=(0, (5, 10)), 
        linestyle='dotted', 
        alpha=0.7)
    if i < 10:
        x_loc = (decile_boundaries[i] + decile_boundaries[i+1]) / 2
        ax.text(x=x_loc, y=ax.get_ylim()[0]*1.2, s=i+1, ha='center')

ax.text(x=ax.get_xlim()[0], y=ax.get_ylim()[0]*1.2, s='Decile', ha='right')
ax.text(0, ax.get_ylim()[1]*1.1, s='Red dot indicates Northumberland Park\nBlack dots indicate MSOAs within 1km of Tottenham Hotspur Stadium\nBlue diamonds indicate the average rank of the borough the Stadium is in', size=8)
# for i, row in la_avg_ranks.iterrows():
#     ax.text(x=row['average_rank'] + nudge_x, 
#             y=row['Stadium'] + nudge_y, 
#             s=row['ladnm'], ha='left')

# Get the mapping from stadium names to y-axis positions
y_labels = ax.get_yticklabels()
y_positions = ax.get_yticks()
label_mapping = {label.get_text(): pos for label, pos in zip(y_labels, y_positions)}

for i, row in la_avg_ranks.iterrows():
    if row['Stadium'] in label_mapping:
        y_pos = label_mapping[row['Stadium']]
        ax.text(x=row['average_rank'] + nudge_x, 
                y=y_pos + nudge_y, 
                s=row['ladnm'], ha='left')


# plt.figtext(0.38, 0.0015, 'Black dots indicate Northumberland Park LSOAs', wrap=False, horizontalalignment='right', fontsize=10)
ax.invert_yaxis()

plt.xlabel('Rank')
# Add title with specific positioning
plt.suptitle(
    'MSOAs within 1km of London Stadiums', fontsize=14, 
    x = 0.7, y=.99, 
    ha='center')  # y controls vertical position

plt.tight_layout()
fig.savefig(os.path.join('..','outputs','london_stadiums_1km.png'), dpi=800)
plt.show()



```


dean court has gone missing

solved. for some reason pop ests from nomis api weren't complete. had to use bulk ts007 instead.

```{python}
test = neighbors_gdf2.loc[neighbors_gdf2['stadium']=='Dean Court']

dean_court_msoas = [
    'E02003182',
    'E02003186',
    'E02003187',
    'E02003190']
    
dc_lookup = lookup.loc[lookup['msoa21cd'].isin(dean_court_msoas)]

dc_relative = relative.loc[relative['msoa_code'].isin(dean_court_msoas)]

dc_pop = pop_ests.loc[pop_ests['msoa21cd'].isin(dean_court_msoas)]
```

An MSOA in Middlesbrough has a child poverty rate higher than the child population. how the heck is this possible? Yeah it seems liek this is really deprived place - 40% of households are deprived in one dimension (https://www.ons.gov.uk/census/maps/choropleth/population/household-deprivation/hh-deprivation/household-is-deprived-in-one-dimension?geoLock=msoa&msoa=E02002498)

It is also the case that the child pvoerty data includes children up to 19 years of age whereas teh population estimates i've used are under 18. maybe this adjustment alone will help the percnetages make more sense. making this change does indeed prevenet percentages > 100 thankfully

More generally I think we need to decide what age groups to focus on.


```{python}
# investigate the high percentage areas
subset = relative2.loc[relative2['percentage']>100]

subset = relative2.loc[relative2['msoa21nm']=="Middlesbrough 003"]

```

CILIF data is using 2011 MSOAs

population estimates are using 2021 MSOAs

cam we get pop ests in 2011 units?

or we look up 2011 pop ests to 2021s


```{python}
# Plot Tottenham agani with deciles this time
haringey = neighbors_gdf2.loc[neighbors_gdf2['stadium'].str.contains('Tottenham')]
tott = tott.to_crs(epsg=27700)
north_park = neighbors_gdf2.loc[neighbors_gdf2['msoa21cd']=='E02000398']
np_centroid = north_park['geometry'].centroid
x, y = np_centroid.x, np_centroid.y
np_rank = north_park['relative_rank'].values[0]
max_rank = int(relative2['rank'].max())
np_decile = north_park['relative_decile'].values[0]

fig, ax = plt.subplots(figsize=[8,8])

# Plot deciles
haringey.plot(
    column='relative_decile', 
    legend=True, legend_kwds={'label':'Decile'}, vmax = 10, vmin = 1,
    ax=ax, alpha=0.5, edgecolor="black")

# Plot stadium
tott.plot(ax=ax, marker='o', color='red', markersize=50, alpha=0.7)

# Titles etc.
plt.title('Child poverty in MSOAs within 1km of Tottenham Hotspur Stadium')
ax.text(x=x, y=y, s='Northumberland Park\nRank: '+ str(int(np_rank)) + '/' + str(max_rank) + '\nDecile: ' + str(int(np_decile)), ha='center', va='center', color='white', size = 9)
ax.text(x=ax.get_xlim()[0],y=ax.get_ylim()[0]-150, s='Red dot indicates Tottenham Hotspur Stadium')
cx.add_basemap(ax, crs = tott.crs, source=cx.providers.OpenStreetMap.Mapnik)
ax.set_axis_off()

fig.savefig(os.path.join('..','outputs','tott_hs_1km_map.png'), dpi=800)

```


```{python}
for area, stadium in zip(las_stads.iloc[:,0], las_stads.iloc[:,1]):
    print(area + ':' + stadium)
    this_area_relative_gdf = neighbors_gdf2.loc[neighbors_gdf2['stadium']==stadium]
    this_area_stadiums_gdf = stadiums_gdf.loc[stadiums_gdf['Stadium'] == stadium]
    this_area_stadiums_gdf = this_area_stadiums_gdf.to_crs(epsg=27700)
    # north_park = neighbors_gdf2.loc[neighbors_gdf2['msoa21cd']=='E02000398']
    # np_centroid = north_park['geometry'].centroid
    # x, y = np_centroid.x, np_centroid.y
    # np_rank = north_park['relative_rank'].values[0]
    # max_rank = int(relative2['rank'].max())
    # np_decile = north_park['relative_decile'].values[0]

    fig, ax = plt.subplots(figsize=[8,8])

    # Plot deciles
    this_area_relative_gdf.plot(
        column='relative_decile', 
        legend=True, legend_kwds={'label':'Decile'}, vmax = 10, vmin = 1,
        ax=ax, alpha=0.5, edgecolor="black")

    # Plot stadium
    this_area_stadiums_gdf.plot(ax=ax, marker='o', color='red', markersize=50, alpha=0.7)

    # Titles etc.
    plt.title('Child poverty in MSOAs within 1km of ' + stadium)
    # ax.text(x=x, y=y, s='Northumberland Park\nRank: '+ str(int(np_rank)) + '/' + str(max_rank) + '\nDecile: ' + str(int(np_decile)), ha='center', va='center', color='white', size = 9)
    ax.text(x=ax.get_xlim()[0],y=ax.get_ylim()[0]-150, s='Red dot indicates ' + stadium +'\nStadium borough: ' + area, va='top')
    cx.add_basemap(ax, crs = tott.crs, source=cx.providers.OpenStreetMap.Mapnik)
    ax.set_axis_off()
```

# investigate missing areas


```{python}
brentford = joined.loc[joined['msoa21nm'].str.contains('Hounslow',na=False)]
```

```{python}
brentford_msoas = neighbors_dict['Brentford Community Stadium']

gdf = msoa_gdf.loc[msoa_gdf['msoa21cd'].isin(brentford_msoas)]

gdf.plot()
```


```{python}
test = neighbors_gdf.loc[neighbors_gdf['msoa21cd'].isin(brentford_msoas)].reset_index(drop=True)
test2 = neighbors_gdf2.loc[neighbors_gdf2['msoa21cd'].isin(brentford_msoas)].reset_index(drop=True)
```


```{python}
test3 = relative.loc[relative['msoa_code']=='E02006973']
```

it looks like the geometry data is there but it's just that there are nas for the cilif data .seems likely this is due to change in msoa that hasn't worked in hte crosswalk


```{python}
test = lookup.loc[lookup['msoa21nm'].str.contains('Hounslow')]
```

hounslow 030 missing? why? could it be one of hte weird translations?